z todo:

## Bakalářka

- vyhodnotit THK1+highest_note nad Orchsetem, porovnat s SourceFilterContoursMelody
- najít si onset detection?!

# schůzka
- výsledky trackování s bleedem vs pYIN
- kvalitativní příklady, vystihující jednotlivé orchestrální jevy
- voicing detection

- evaluace mého nejlepšího modelu na mono Pachabelovi: 60% vs 80% pYIN

- precision nejvyššího tónu na >=3 polyfonní klasice v MedleyDB (21 skladeb) je 85%
	- sice recall 100% a počet anotovaných hlasů není roven celkovému počtu hlasů
	- poslouchal jsem Orchset a od ucha - většina melodie prostě je nejvýš
		- stejnak to není úplně reprezentativní, ukázky jsou schválně vybrané tak, aby tam byla melodie jednoznačná
	- ale co takhle zkusit trénovat na MusicNetu, kde beru jako melodii nejvyšší nástroj/tón?
		- výběr melodie z dat se stává hyperparametrem
	- případně trénovat multif0 a vzít tu nejvyšší

- vyhodnotit THK1+highest_note nad Orchsetem (a hlavně nad MEDLEY), porovnat s SourceFilterContoursMelody

- detekovat onsety
- zkusit ty spektrogramy
- oddělit voicing a do tabulek

========

# Shrnutí

do 17. 5. 2019
Odevzdání bakalářských prací pro letní termín bakalářských státních závěrečných zkoušek 

## Otázky

- jak má vlastně vypadat sekce related work?

*Minimální odevzdatelná bakalářka*
Texty nezávislé na experimentální části:

- Motivační a obecné texty
    - úvod (již drobně rozepsané, nic moc ale, viz `poznamky.md`), ve zkratce:
        - proč extrahovat melodii
        - proč je to těžké
    - definice a vysvětlení
        - technické popisy
            - spektrogramy?
            - machine/deep learning?
            - metriky a metody evaluace

        - (Bittner):
            - Pitch vs. Fundamental frequency
            - Pitch perception
            - f0 representations
            - Voicing
    
- Related work
- Datasets (také drobně rozepsané)
    - popisy jednotlivých dostupných datasetů
    - diskuse o syntetických datech

Texty závislé na experimentální části
- Struktura experimentu?
- vlastně nevím, co je přínos mé bakalářky
    - v poznámkách ze schůzky - monopitch tracker robustní vůči melodickému šumu?

*další TODOs:*

- udělat si pořádně rešerši, přibylo víc článků za rok 2018
    - v průběhu opravdu psát sekci Related work, nikoli jen poznámky.
- trénování trvá dlouho - jelikož už mám připravený dobrý framework pro spouštění experimentů, mohl bych několikrát do týdne spustit trénování nějakých modelů

myslím, že bych měl udělat nějaké experimenty. V tuhle chvíli mi připadá, že nemám moc výsledků do experimentální části.
- Oracle AMT -> MIDI Melody
    - spustit různé multif0 algoritmy na MedleyDB a zjistit, jestli přepíšou hudbu včetně hlavní melodie
        - Pokud bychom měli výsledky rozumně vysoké (tj. vyšší než melody tracking), pak má smysl dělat modul AMT->ME

## Textová část bakalářky

### Struktura


## Praktická část

### Framework pro experimenty

https://github.com/kukas/music-transcription
- Dataset handling
    - [MusicNet](https://homes.cs.washington.edu/~thickstn/musicnet.html) dataset loading and automatic resampling. Other datasets such as MIREX 2007 MultiF0, Bach10, Su, MedleyDB are not supported yet but should be easy to add.
    - Slicing of the dataset audio for creating small testing subsets.
    - Automatic processing of a dataset - spectrogram precomputation and management
- Visualization tools for examining the model output.
    - Piano roll for comparison between the _gold truth_ and _estimation_
    - Interactive audio output for Jupyter notebooks
    - STFT and constant-Q spectrograms (using `librosa`)
- Tensorflow model skeleton
    - Training, evaluation and inference functions
    - Detailed evaluation summary in Tensorboard
        - Evaluation of the testing set using `mir_eval`, implementation of basic metrics in Tensorflow for training information
        - Visual qualitative example = piano roll of a transcription
    - Saving the model weights and topology

### Melody extraction toolkit

https://github.com/kukas/melody_extraction_toolkit
- prohlížení datasetů v MIREX formátu
    - pianoroll anotace a predikce ze zvoleného algoritmu
    - zvuková syntéza referenční anotace/predikované anotace

### Experimenty

Hypotéza: Výška tónu poměrně dobře predikuje 

#### Anotace MusicNet


### Datasety

- MusicNet
    - 34 hodin
    - MIDI všech melodických nástrojů (rozlišené nástroje)
    - způsob: Dynamic time warp MIDI souborů k nahrávkám
    - bez vyznačení hlavní melodie
- MedleyDB
    - TODO - nová verze
- Li Su
- Bach10
- MAPS
    - TODO A-MAPS

- Orchset
    - 23 minut hudby
    - MIDI hlavní melodie

- WEIMAR JAZZ DATABASE (WJAZZD)
    - jazzová sóla

Synth datasets: http://synthdatasets.weebly.com/
https://ismir2017.smcnus.org/wp-content/uploads/2017/10/164_Paper.pdf
AN ANALYSIS/SYNTHESIS FRAMEWORK FOR AUTOMATIC F0R ANNOTATION OF MULTITRACK DATASETS
- ​The datasets are intended for research on monophonic, melody, bass, and multiple f0 estimation (pitch tracking), and include:

- MDB-melody-synth
- 65 songs from the MedleyDB dataset in which the melody track has been resynthesized to obtain a perfect melody f0 annotation using the analysis/synthesis method described in the paper.

- MDB-bass-synth
- 71 songs from the MedleyDB dataset in which the bass track has been resynthesized to obtain a perfect bass f0 annotation using the analysis/synthesis method described in the paper.

- MDB-mf0-synth
- 85 songs from the MedleyDB dataset in which polyphonic pitched instruments (such as piano and guitar) have been removed and all monophonic pitched instruments (such as bass and voice) have been resynthesized to obtain perfect f0 annotations using the analysis/synthesis method described in the paper.

- MDB-stem-synth
- 230 solo stems (tracks) from the MedleyDB dataset spanning a variety of musical instruments and voices, which have been resynthesized to obtain a perfect f0 annotation using the analysis/synthesis method described in the paper.

- Bach10-mf0-synth
- 10 classical music pieces (four-part J.S. Bach chorales) from the Bach10 dataset where each instrument (bassoon, clarinet, saxophone and violin) has been resynthesized to obtain perfect f0 annotations using the analysis/synthesis method described in the paper.

#### Syntéza
- The Lakh MIDI Dataset v0.1

The Lakh MIDI dataset is a collection of 176,581 unique MIDI files, 45,129 of which have been matched and aligned to entries in the Million Song Dataset. Its goal is to facilitate large-scale music information retrieval, both symbolic (using the MIDI files alone) and audio content-based (using information extracted from the MIDI files as annotations for the matched audio files).

- https://www.classicalarchives.com/

### Experimenty

### Připravené algoritmy

#### Mono f0
- CREPE
    - monopitch tracker
- PYIN
    - state of the art ve většině prací

#### Multi f0
- Thickstun - MusicNet baselines


#### Melody f0
- ISMIR2016: MELODY EXTRACTION ON VOCAL SEGMENTS USING MULTI-COLUMN DEEP NEURAL NETWORKS
- SourceFilterContoursMelody
    - J. Bosch, E. Gómez, "Melody extraction based on a source-filter model using pitch contour selection", in Proc. 13th Sound and Music Computing Conference (SMC 2016). Hamburg, Germany, 2016. p.67-74
- singing_voice_separation_and_melody_extraction.sh
    - 2016 Singing Voice Separation and Vocal F0 Estimation Based on Mutual Combination of Robust Principal Component Analysis and Subharmonic Summation