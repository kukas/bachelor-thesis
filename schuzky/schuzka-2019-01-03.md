# Shrnutí

do 17. 5. 2019
Odevzdání bakalářských prací pro letní termín bakalářských státních závěrečných zkoušek 

dopříště 21.1.
    - online text - buď Overleaf (v1 má integraci s Gitem) nebo Github
    - vyhrabat split MedleyDB (porovnat s related work)
    - evaluační pipeline + spustit ty experimenty, co mám
        + stávající metody pro ME
        + a pro baselines


    - ilustrační příklady
        - orchestrální i neorchestrální
            - metody z related work fungují na neorch.
        - jeden hlas
        - melodie nahoře
        - melodie dole
        - melodie uprostřed

        - vlastnosti melodie
            - stabilní dlouhý tóny (a kolem doprovod)
            - něco proměnlivého
        - potichu/nahlas

- jak má vlastně vypadat sekce related work?

*Minimální odevzdatelná bakalářka*
Texty nezávislé na experimentální části:

- Motivační a obecné texty
    - úvod (již drobně rozepsané, nic moc ale, viz `poznamky.md`), ve zkratce:
        - proč extrahovat melodii
        - proč je to těžké
    - definice a vysvětlení
        - technické popisy
            - spektrogramy?
            - machine/deep learning?
            - metriky a metody evaluace

        - (Bittner):
            - Pitch vs. Fundamental frequency
            - Pitch perception
            - f0 representations
            - Voicing
    
- Related work
- Datasets (také drobně rozepsané)
    - popisy jednotlivých dostupných datasetů
    - diskuse o syntetických datech
    - diskuse o MDB-synth  
        - artefakty u začátků a konců tónů hlavně u klasiky

Texty závislé na experimentální části
- Struktura experimentu?
- vlastně nevím, co je přínos mé bakalářky
    - v poznámkách ze schůzky - monopitch tracker robustní vůči melodickému šumu?

*další TODOs:*

- udělat si pořádně rešerši, přibylo víc článků za rok 2018
    - v průběhu opravdu psát sekci Related work, nikoli jen poznámky.
- trénování trvá dlouho - jelikož už mám připravený dobrý framework pro spouštění experimentů, mohl bych několikrát do týdne spustit trénování nějakých modelů?

myslím, že bych měl udělat nějaké experimenty. V tuhle chvíli mi připadá, že nemám moc výsledků do experimentální části.
- Oracle AMT -> MIDI Melody
    - spustit různé multif0 algoritmy na MedleyDB a zjistit, jestli přepíšou hudbu včetně hlavní melodie
        - Pokud bychom měli výsledky rozumně vysoké (tj. vyšší než melody tracking), pak má smysl dělat modul AMT->ME

- kvalitativní příklady, vystihující jednotlivé orchestrální jevy
    - něco takového mám v `music-transcription`, ale dost basic.

## Textová část bakalářky

### Struktura

## Praktická část

### Framework pro experimenty

https://github.com/kukas/music-transcription
- Dataset handling
    - [MusicNet](https://homes.cs.washington.edu/~thickstn/musicnet.html) dataset loading and automatic resampling. Other datasets such as MIREX 2007 MultiF0, Bach10, Su, MedleyDB are not supported yet but should be easy to add.
    - Slicing of the dataset audio for creating small testing subsets.
    - Automatic processing of a dataset - spectrogram precomputation and management
- Visualization tools for examining the model output.
    - Piano roll for comparison between the _gold truth_ and _estimation_
    - Interactive audio output for Jupyter notebooks
    - STFT and constant-Q spectrograms (using `librosa`)
- Tensorflow model skeleton
    - Training, evaluation and inference functions
    - Detailed evaluation summary in Tensorboard
        - Evaluation of the testing set using `mir_eval`, implementation of basic metrics in Tensorflow for training information
        - Visual qualitative example = piano roll of a transcription
    - Saving the model weights and topology

### Melody extraction toolkit

https://github.com/kukas/melody_extraction_toolkit
- prohlížení datasetů v MIREX formátu
    - pianoroll anotace a predikce ze zvoleného algoritmu
    - zvuková syntéza referenční anotace/predikované anotace

### Experimenty (provedené nebo nedodělané)

Hypotéza: Výška tónu poměrně dobře predikuje melodii

- baseline MedleyDB multif0->f0 pomocí nejvyššího tónu
    - problém je, že MedleyDB nemá kompletní přepis multif0, ale pouze těch melodických, monofonních. Tudíž tahle baseline nemá vypovídající hodnotu, protože je to moc zjednodušený případ oproti realitě.
    - precision nejvyššího tónu na >=3 polyfonní klasice v MedleyDB (21 skladeb) je 85%
    	- sice recall 100% a počet anotovaných hlasů není roven celkovému počtu hlasů

- baseline spustit nějakou metodu AMT na MedleyDB/Orchset, multif0->f0 pomocí nejvyššího tónu
    - THK1
    - porovnat s SourceFilterContoursMelody

- navrhuji: spustit nějakou metodu AMT na MedleyDB/Orchset, a pak oracle multif0->f0 (tj. zjistíme, jestli je hlavní melodie vůbec na výstupu)
    - spouštím ismir2017-deepsalience, jakožto jednu z nejnovějších metod, volně dostupnou a pravděpodobně state-of-the-art
    - výsledky na mamutovi - `bakalarka-experimenty/multif0->melody/oracle-multif0`


Jednoduché baseline
- raw audio + multimelody -> melody
- raw audio -> melody
- raw audio -> multimelody (v rámci projektu na Python)
- spectrograms -> multimelody (v rámci projektu na Python)

- zkusit monopitch tracking na MedleyDB tracky s bleedem, porovnat s pYIN
- evaluace mého nejlepšího modelu na mono Pachabelovi: 60% vs 80% pYIN (a to nemá bleed, takže hodně špatný)


vygenerovat si výstupy pro datasety: MedleyDB, Orchset, MDB-synth

./melody_extraction_toolkit/algorithms/SourceFilterContoursMelody/src/medley/SourceFilterContoursMelody_evaluation.ipynb
    - vyhodnocení SourceFilterContoursMelody na orchestrálním subsetu MedleyDB
    - výsledky na orchestrálním, testovacím splitu z medleydb:
        - BG1
        - Voicing Recall 0.9135678559196531
        - Voicing False Alarm 	 0.7916667893352656
        - Raw Pitch Accuracy 	 0.4816615332248101
        - Raw Chroma Accuracy 	 0.5765505898948432
        - Overall Accuracy 	 0.3833178224648009

        BG2
        - Voicing Recall 	 0.7489142285626881
        - Voicing False Alarm 	 0.4243912374929407
        - Raw Pitch Accuracy 	 0.47646987887020426
        - Raw Chroma Accuracy 	 0.5675367125311481
        - Overall Accuracy 	 0.4491012577798134

        CMB
        - Voicing Recall 	 0.5450625052680182
        - Voicing False Alarm 	 0.2576412028138208
        - Raw Pitch Accuracy 	 0.4452263404057648
        - Raw Chroma Accuracy 	 0.5309153306897897
        - Overall Accuracy 	 0.4211866563024154


./algorithms/crepe/MusicDelta_Vivaldi_STEMS/crepe-eval-2.ipynb
    zkouška staré verze CREPE na jednom stemu z Vivaldiho (medleydb)

./datasets/medley/medley-basic-spectral-baseline.ipynb
    ukázka na jednom polyfonním příkladě, že extrakce melodie nelze vyřešit hledáním nejsilnější frekvence ve spektrogramu

./melody_extraction/a->m-wavenet+lstm.ipynb
    - znovu wavenet, s lstm vrstvou nad výstupem konvolucí, nicméně stejně malý kontext...
./melody_extraction/a->m-wavenet+lstm-Copy1.ipynb
    - wavenet s lstm a větším kontextem

./test/a->m- 05-01_011002-bs64.ipynb

./test/a->m-05-02_001356-bs64-Copy1.ipynb
    - nejlepší vytvořená
        - na orchestrálním, testovacím splitu z medleydb dosahuje 46%OA, 40%RPA. 
    - architektura inspirovaná WaveNetem
        - nicméně konvoluce mají kernel šířky 3
    - dilatované konvoluce až do dilatation rate 256
    - spolu s tím zdvojnásobování filtrů po několika vrstvách
        - před zdvojnásobovováním residuální konekce
    - poslední vrstvy batch_norm (dal bych všechny, ale nevešlo se do paměti)
    - mezi poslední konvolucí a konvolucí predikující noty je dropout
    test_names = ["MusicDelta_Pachelbel", "MusicDelta_GriegTrolltog", "MusicDelta_Vivaldi", "MusicDelta_Beethoven", "MusicDelta_InTheHalloftheMountainKing", "Debussy_LenfantProdigue", "JoelHelander_Definition"]
 
./test/medley->melody_2-context.ipynb
    - baseline 1 hidden vrstva
./test/medley->melody_2.ipynb
    - model s různě nastavenými "uši" - na dilatované konvoluce nad audiovstupem lze nahlížet jako na resamplované části audia
./test/medley-clean-subset.ipynb
    - ručně jsem prošel orchestrální MedleyDB a vybral stemy s kompletní (nebo téměř kompletní) anotací. Vytvořil jsem z těchto stemů čisté mixy, MedleyDB-multif0, nicméně jedná se pouze o 28 minut hudby...
./test/medley-multif0-to-melody.ipynb
./test/pyin-vs-medley.ipynb
    - pYIN monopitch tracker spuštěný na orchestrálním subsetu MedleyDB, zmapované nedostatky pYINu a MedleyDB
./test/crepe-vs-medley.ipynb
    - vyhodnocení starého CREPE na orch. subsetu MedleyDB


chybí mi koncepčně udělané evaluace.
- chtělo by to implementovat evaluaci nad MIREX datasety, MedleyDB, MedleyDB-Orchestral, MDB-synth-melody
- kvalitativní analýza - orchestrální jevy

### Datasety

   |                 | MedleyDB | Orchset | MusicNet | URMP | MDB-synth | WJAZZD? | DSD100
1. | mix audio       | ✔        | ✔       | ✔        | ✔    | ✔         | ✔       | ✔
2. | stem audio      | ✔ **     | x       | x        | ✔    | ✔         | x       | ✔
3. | Multi f0        | *        | x       | x        | ✔    | ✔         | x       | x
3. | MIDI            | *        | x       | ✔        | ✔    | ✔         | x       | x
4. | Stem MIDI       | ✔        | x       | ✔        | ✔    | ✔         | x       | x
5. | Stem priority   | ✔        | x       | x        | x    | ✔         | x       | x
6. | Melody f0       | ✔        | x       | x        | x    | ✔         | ✔?      | x
7. | Melody MIDI     | ✔        | ✔       | x        | x    | ✔         | ✔       | x

   | Hours of audio  | 7.3      | 23min   | 34       | 1.3  | 4.65      | ??      | 7

* pouze monofonické nástroje, pouze melodie (tj. doprovod není anotován)
** část stemů s větším či menším bleedem (toto je anotované)

pěkný podobný seznam datasetů: https://arxiv.org/pdf/1612.08727.pdf

#### Nové datasety
- URMP
    - délka 1,3h 
    - klasika
    - kompletní anotace a multitrack

- WEIMAR JAZZ DATABASE (WJAZZD)
    - jazzová sóla

Synth datasets: http://synthdatasets.weebly.com/
https://ismir2017.smcnus.org/wp-content/uploads/2017/10/164_Paper.pdf
AN ANALYSIS/SYNTHESIS FRAMEWORK FOR AUTOMATIC F0R ANNOTATION OF MULTITRACK DATASETS
- ​The datasets are intended for research on monophonic, melody, bass, and multiple f0 estimation (pitch tracking), and include:

- MDB-melody-synth
- 65 songs from the MedleyDB dataset in which the melody track has been resynthesized to obtain a perfect melody f0 annotation using the analysis/synthesis method described in the paper.

- MDB-bass-synth
- 71 songs from the MedleyDB dataset in which the bass track has been resynthesized to obtain a perfect bass f0 annotation using the analysis/synthesis method described in the paper.

- MDB-mf0-synth
- 85 songs from the MedleyDB dataset in which polyphonic pitched instruments (such as piano and guitar) have been removed and all monophonic pitched instruments (such as bass and voice) have been resynthesized to obtain perfect f0 annotations using the analysis/synthesis method described in the paper.

- MDB-stem-synth
- 230 solo stems (tracks) from the MedleyDB dataset spanning a variety of musical instruments and voices, which have been resynthesized to obtain a perfect f0 annotation using the analysis/synthesis method described in the paper.

- Bach10-mf0-synth
- 10 classical music pieces (four-part J.S. Bach chorales) from the Bach10 dataset where each instrument (bassoon, clarinet, saxophone and violin) has been resynthesized to obtain perfect f0 annotations using the analysis/synthesis method described in the paper.


- MedleyDB
    - Přidali nové multitracky

#### Staré datasety

- MusicNet
    - 34 hodin
    - MIDI všech melodických nástrojů (rozlišené nástroje)
    - způsob: Dynamic time warp MIDI souborů k nahrávkám
    - bez vyznačení hlavní melodie
- Li Su
    - multif0, 10 mixů
    - 5 min
- Bach10
    - multif0, 10 chorálů
    - 5.5 min
- MAPS
    - 18.6h !
    - ale jenom piano

- Orchset
    - 23 minut hudby
    - MIDI hlavní melodie

#### Syntéza
- The Lakh MIDI Dataset v0.1

The Lakh MIDI dataset is a collection of 176,581 unique MIDI files, 45,129 of which have been matched and aligned to entries in the Million Song Dataset. Its goal is to facilitate large-scale music information retrieval, both symbolic (using the MIDI files alone) and audio content-based (using information extracted from the MIDI files as annotations for the matched audio files).

- https://www.classicalarchives.com/

### Experimenty

### Připravené algoritmy

#### Mono f0
- CREPE
    - monopitch tracker
- PYIN
    - state of the art ve většině prací

#### Multi f0
- ismir2017-deepsalience

#### Melody f0
- ISMIR2016: MELODY EXTRACTION ON VOCAL SEGMENTS USING MULTI-COLUMN DEEP NEURAL NETWORKS
- SourceFilterContoursMelody
    - J. Bosch, E. Gómez, "Melody extraction based on a source-filter model using pitch contour selection", in Proc. 13th Sound and Music Computing Conference (SMC 2016). Hamburg, Germany, 2016. p.67-74
- singing_voice_separation_and_melody_extraction.sh
    - 2016 Singing Voice Separation and Vocal F0 Estimation Based on Mutual Combination of Robust Principal Component Analysis and Subharmonic Summation

Zvážit?
https://www.upf.edu/web/mtg/melodia = Salamon 2012 J. Salamon and E. Gomez, “Melody extraction from polyphonic music ´
signals using pitch contour characteristics,” IEEE Trans. on Audio,
Speech and Language Processing, vol. 20, no. 6, Aug. 2012.

Protože se v paperech pro srovnání používá celkem často
http://www.justinsalamon.com/news/melody-extraction-in-python-with-melodia

Durrieu: http://www.durrieu.ch/research/jstsp2010.html
    - ODKAZ separateLeadStereo.zip: the programs and scripts implementing the proposed systems: melody estimation, VIMM and VUIMM to separate the lead instrument from the accompaniment.
    - má totiž parádní výsledky na Orchsetu kuk: Evaluation and combination of pitch estimation methods for melody extraction in symphonic classical music