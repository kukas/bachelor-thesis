Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Salamon2017,
author = {Salamon, Justin and Bittner, Rachel M. and Bonada, Jordi and Bosch, Juan J. and Gomez, Emilia and juan pablo Bello},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Salamon et al. - 2017 - An AnalysisSynthesis Framework for Automatic F0 Annotation of Multitrack Datasets.pdf:pdf},
journal = {Proceedings of the International Society for Music Information Retrieval {\{}(ISMIR){\}} Conference},
keywords = {dataset},
mendeley-tags = {dataset},
pages = {71--78},
title = {{An Analysis/Synthesis Framework for Automatic F0 Annotation of Multitrack Datasets}},
year = {2017}
}
@article{DeCheveigne2002,
abstract = {An algorithm is presented for the estimation of the fundamental frequency (F0) of speech or musical sounds. It is based on the well-known autocorrelation method with a number of modifications that combine to prevent errors. The algorithm has several desirable features. Error rates are about three times lower than the best competing methods, as evaluated over a database of speech recorded together with a laryngograph signal. There is no upper limit on the frequency search range, so the algorithm is suited for high-pitched voices and music. The algorithm is relatively simple and may be implemented efficiently and with low latency, and it involves few parameters that must be tuned. It is based on a signal model (periodic signal) that may be extended in several ways to handle various forms of aperiodicity that occur in particular applications. Finally, interesting parallels may be drawn with models of auditory processing.},
author = {de Cheveign{\'{e}}, Alain and Kawahara, Hideki},
doi = {10.1121/1.1458024},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/8ddafe2e3de9428a6dcb89165a4188c7eb762724.pdf:pdf},
isbn = {0001-4966 (Print)},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
number = {4},
pages = {1917--1930},
pmid = {12002874},
title = {{YIN, a fundamental frequency estimator for speech and music}},
url = {http://audition.ens.fr/adc/pdf/2002{\_}JASA{\_}YIN.pdf},
volume = {111},
year = {2002}
}
@article{Hermes1988,
abstract = {Hermes, D. J. (1988). Measurement of pitch by subharmonic summation. The journal of the acoustical society of America, 83(1), 257-264.},
author = {Hermes, Dik J.},
doi = {10.1121/1.396427},
file = {:home/jirka/Sta{\v{z}}en{\'{e}}/Hermes{\_}1988{\_}SHS.pdf:pdf},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
number = {1},
pages = {257--264},
title = {{Measurement of pitch by subharmonic summation}},
url = {http://asa.scitation.org/doi/10.1121/1.396427},
volume = {83},
year = {1988}
}
@article{Kim2018,
abstract = {The task of estimating the fundamental frequency of a monophonic sound recording, also known as pitch tracking, is fundamental to audio processing with multiple applications in speech processing and music information retrieval. To date, the best performing techniques, such as the pYIN algorithm, are based on a combination of DSP pipelines and heuristics. While such techniques perform very well on average, there remain many cases in which they fail to correctly estimate the pitch. In this paper, we propose a data-driven pitch tracking algorithm, CREPE, which is based on a deep convolutional neural network that operates directly on the time-domain waveform. We show that the proposed model produces state-of-the-art results, performing equally or better than pYIN. Furthermore, we evaluate the model's generalizability in terms of noise robustness. A pre-trained version of CREPE is made freely available as an open-source Python module for easy application.},
archivePrefix = {arXiv},
arxivId = {arXiv:1802.06182v1},
author = {Kim, Jong Wook and Salamon, Justin and Li, Peter and Bello, Juan Pablo},
doi = {10.1109/ICASSP.2018.8461329},
eprint = {arXiv:1802.06182v1},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/b39b2638c4496d2813898ff6b5ae848eec840404.pdf:pdf},
isbn = {9781538646588},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {Convolutional neural network,Pitch estimation},
pages = {161--165},
title = {{Crepe: A Convolutional Representation for Pitch Estimation}},
url = {https://arxiv.org/pdf/1802.06182.pdf},
volume = {2018-April},
year = {2018}
}
@article{Bittner2017,
abstract = {Estimating fundamental frequencies in polyphonic music remains a notoriously difficult task in Music Information Retrieval. While other tasks, such as beat tracking and chord recognition have seen improvement with the appli-cation of deep learning models, little work has been done to apply deep learning methods to fundamental frequency related tasks including multi-f 0 and melody tracking, pri-marily due to the scarce availability of labeled data. In this work, we describe a fully convolutional neural network for learning salience representations for estimating fundamen-tal frequencies, trained using a large, semi-automatically generated f 0 dataset. We demonstrate the effectiveness of our model for learning salience representations for both multi-f 0 and melody tracking in polyphonic audio, and show that our models achieve state-of-the-art performance on several multi-f 0 and melody datasets. We conclude with directions for future research.},
author = {Bittner, Rachel M. and Mcfee, Brian and Salamon, Justin and Li, Peter and Bello, Juan P.},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bittner et al. - 2017 - Deep Salience Representations for F0 Estimation in Polyphonic Music.pdf:pdf},
journal = {Ismir},
keywords = {melody extraction,multiple f0 estimation},
mendeley-tags = {melody extraction,multiple f0 estimation},
pages = {23--27},
title = {{Deep Salience Representations for F0 Estimation in Polyphonic Music}},
url = {https://bmcfee.github.io/papers/ismir2017{\_}salience.pdf},
year = {2017}
}
@article{Dressler2009,
author = {Dressler, Karin},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/f92373be9eedcc252e00a579b012c232480ab1ce.pdf:pdf},
journal = {Evaluation eXchange (MIREX)},
pages = {1--3},
title = {{Audio melody extraction for mirex 2009}},
url = {http://www.idmt.fraunhofer.de/content/dam/idmt/de/Dokumente/Produktflyer/QbH/white{\_}paper{\_}fraunhofer{\_}idmt{\_}audio{\_}melody{\_}extraction{\_}mirex{\_}2009.pdf},
year = {2009}
}
@article{He2015,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
eprint = {1512.03385},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2c03df8b48bf3fa39054345bafabfeff15bfd11d.pdf:pdf},
title = {{Deep Residual Learning for Image Recognition}},
url = {http://arxiv.org/abs/1512.03385},
year = {2015}
}
@article{Oord2016a,
abstract = {This work explores conditional image generation with a new image density model based on the PixelCNN architecture. The model can be conditioned on any vector, including descriptive labels or tags, or latent embeddings created by other networks. When conditioned on class labels from the ImageNet database, the model is able to generate diverse, realistic scenes representing distinct animals, objects, landscapes and structures. When conditioned on an embedding produced by a convolutional network given a single image of an unseen face, it generates a variety of new portraits of the same person with different facial expressions, poses and lighting conditions. We also show that conditional PixelCNN can serve as a powerful decoder in an image autoencoder. Additionally, the gated convolutional layers in the proposed model improve the log-likelihood of PixelCNN to match the state-of-the-art performance of PixelRNN on ImageNet, with greatly reduced computational cost.},
archivePrefix = {arXiv},
arxivId = {1606.05328},
author = {van den Oord, Aaron and Kalchbrenner, Nal and Vinyals, Oriol and Espeholt, Lasse and Graves, Alex and Kavukcuoglu, Koray},
eprint = {1606.05328},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/d0aa7887e966dc34116e7db1fa714c36fce0adce.pdf:pdf},
title = {{Conditional Image Generation with PixelCNN Decoders}},
url = {http://arxiv.org/abs/1606.05328},
year = {2016}
}
@article{Slaney1993,
author = {Slaney, Malcolm and Lyon, Richard F},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/42ad88836ecc471cd0667843ecc34098264363aa.pdf:pdf},
pages = {95--116},
title = {{On the Importance of Time}},
url = {https://pdfs.semanticscholar.org/42ad/88836ecc471cd0667843ecc34098264363aa.pdf},
year = {1993}
}
@article{Ioffe2015,
abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9{\%} top-5 validation error (and 4.8{\%} test error), exceeding the accuracy of human raters.},
archivePrefix = {arXiv},
arxivId = {1502.03167},
author = {Ioffe, Sergey and Szegedy, Christian},
eprint = {1502.03167},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/c1baed41e4bc9401b1b2ec8ef55ba45543f7a1a3.pdf:pdf},
title = {{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}},
url = {http://arxiv.org/abs/1502.03167},
year = {2015}
}
@misc{Benetos2019,
author = {Benetos, E.},
file = {:home/jirka/08588423.pdf:pdf},
title = {{Automatic Music Transcription: An Overview}},
url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp={\&}arnumber=8588423},
year = {2019}
}
@article{Cancela2008,
abstract = {In this work a melody extraction technique is introduced to the MIREX 2008 campaign. The task's objective consists in estimating the pitch of the main melody in polyphonic au- dio. The proposed method is based in three separate mod- ules. The first one evaluates the presence of quasi-stationary harmonic sources in a frame level based approach. Next, sources are tracked using the output of the first step, obtain- ing a set of pitch contours. In the last step, it is decided whether or not each contour at each time is part of the main melody.},
author = {Cancela, Pablo},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/226ea7b870fd229149fae2e6c8b15d8a3d4f9bb8.pdf:pdf},
journal = {4th Music Information Retrieval Evaluation eXchange},
keywords = {ismir,multiresolution spectrum,tracking pitch},
title = {{Tracking melody in polyphonic audio.}},
url = {https://pdfs.semanticscholar.org/226e/a7b870fd229149fae2e6c8b15d8a3d4f9bb8.pdf},
year = {2008}
}
@article{Kingma2014,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik P. and Ba, Jimmy},
eprint = {1412.6980},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/aeea02a93d674e0a044a8715e767f3a372582604.pdf:pdf},
pages = {1--15},
title = {{Adam: A Method for Stochastic Optimization}},
url = {http://arxiv.org/abs/1412.6980},
year = {2014}
}
@article{Thickstun2016,
abstract = {This paper introduces a new large-scale music dataset, MusicNet, to serve as a source of supervision and evaluation of machine learning methods for music research. MusicNet consists of hundreds of freely-licensed classical music recordings by 10 composers, written for 11 instruments, together with instrument/note annotations resulting in over 1 million temporal labels on 34 hours of chamber music performances under various studio and microphone conditions. The paper defines a multi-label classification task to predict notes in musical recordings, along with an evaluation protocol, and benchmarks several machine learning architectures for this task: i) learning from spectrogram features; ii) end-to-end learning with a neural net; iii) end-to-end learning with a convolutional neural net. These experiments show that end-to-end models trained for note prediction learn frequency selective filters as a low-level representation of audio.},
archivePrefix = {arXiv},
arxivId = {1611.09827},
author = {Thickstun, John and Harchaoui, Zaid and Kakade, Sham},
eprint = {1611.09827},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Thickstun, Harchaoui, Kakade - 2016 - Learning Features of Music from Scratch.pdf:pdf},
pages = {1--14},
title = {{Learning Features of Music from Scratch}},
url = {http://arxiv.org/abs/1611.09827},
year = {2016}
}
@article{Sutskever2014,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different “thinned” networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets},
author = {Sutskever, Ilya and Hinton, Geoffrey and Krizhevsky, Alex and Salakhutdinov, Ruslan R},
journal = {Journal of Machine Learning Research},
keywords = {deep learning,model combination,neural networks,regularization},
pages = {1929--1958},
title = {{Dropout : A Simple Way to Prevent Neural Networks from Overfitting}},
volume = {15},
year = {2014}
}
@article{Ikemiya2016,
abstract = {This paper presents a new method of singing voice analysis that performs mutually-dependent singing voice separation and vocal fundamental frequency (F0) estimation. Vocal F0 estimation is considered to become easier if singing voices can be separated from a music audio signal, and vocal F0 contours are useful for singing voice separation. This calls for an approach that improves the performance of each of these tasks by using the results of the other. The proposed method first performs robust principal component analysis (RPCA) for roughly extracting singing voices from a target music audio signal. The F0 contour of the main melody is then estimated from the separated singing voices by finding the optimal temporal path over an F0 saliency spectrogram. Finally, the singing voices are separated again more accurately by combining a conventional time-frequency mask given by RPCA with another mask that passes only the harmonic structures of the estimated F0s. Experimental results showed that the proposed method significantly improved the performances of both singing voice separation and vocal F0 estimation. The proposed method also outperformed all the other methods of singing voice separation submitted to an international music analysis competition called MIREX 2014.},
author = {Ikemiya, Yukara and Itoyama, Katsutoshi and Yoshii, Kazuyoshi},
doi = {10.1109/TASLP.2016.2577879},
file = {:home/jirka/07486082.pdf:pdf},
issn = {23299290},
journal = {IEEE/ACM Transactions on Audio Speech and Language Processing},
keywords = {Robust principal component analysis (RPCA),singing voice separation,subharmonic summation (SHS),vocal F0 estimation},
number = {11},
pages = {2084--2095},
publisher = {IEEE},
title = {{Singing Voice Separation and Vocal F0 Estimation Based on Mutual Combination of Robust Principal Component Analysis and Subharmonic Summation}},
volume = {24},
year = {2016}
}
@article{Thickstun2018,
abstract = {This paper explores a variety of models for frame-based music transcription, with an emphasis on the methods needed to reach state-of-the-art on human recordings. The translation-invariant network discussed in this paper, which combines a traditional filterbank with a convolutional neural network, was the top-performing model in the 2017 MIREX Multiple Fundamental Frequency Estimation evaluation. This class of models shares parameters in the log-frequency domain, which exploits the frequency invariance of music to reduce the number of model parameters and avoid overfitting to the training data. All models in this paper were trained with supervision by labeled data from the MusicNet dataset, augmented by random label-preserving pitch-shift transformations.},
archivePrefix = {arXiv},
arxivId = {1711.04845},
author = {Thickstun, John and Harchaoui, Zaid and Foster, Dean P. and Kakade, Sham M.},
doi = {10.1109/ICASSP.2018.8461686},
eprint = {1711.04845},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/cdf71b6c544d1bf0063b891086bdffdfac8e2112.pdf:pdf},
isbn = {9781538646588},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {Convolutional neural networks,Invariances,Learning,Music information retrieval},
pages = {2241--2245},
title = {{Invariances and Data Augmentation for Supervised Music Transcription}},
url = {https://arxiv.org/pdf/1711.04845.pdf},
volume = {2018-April},
year = {2018}
}
@article{Bosch2016,
abstract = {The extraction of pitch information is arguably one of the most important tasks in automatic music description systems. However, previous research and evaluation datasets dealing with pitch estimation focused on relatively limited kinds of musical data. This work aims to broaden this scope by addressing symphonic western classical music recordings, focusing on pitch estimation for melody extraction. This material is characterized by a high number of overlapping sources, and by the fact that the melody may be played by different instrumental sections, often alternating within an excerpt. We evaluate the performance of eleven state-of-the-art pitch salience functions, multipitch estimation and melody extraction algorithms when determining the sequence of pitches corresponding to the main melody in a varied set of pieces. An important contribution of the present study is the proposed evaluation framework, including the annotation methodology, generated dataset and evaluation metrics. The results show that the assumptions made by certain methods hold better than others when dealing with this type of music signal, leading to a better performance. Additionally, we propose a simple method for combining the output of several algorithms, with promising results.},
author = {Bosch, Juan J. and Marxer, Ricard and G{\'{o}}mez, Emilia},
doi = {10.1080/09298215.2016.1182191},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bosch, Marxer, G{\'{o}}mez - 2016 - Evaluation and combination of pitch estimation methods for melody extraction in symphonic classical music.pdf:pdf},
issn = {17445027},
journal = {Journal of New Music Research},
keywords = {dataset,evaluation,melody extraction,music analysis,music information retrieval,perception,review,symphonic music},
mendeley-tags = {dataset,review},
number = {2},
pages = {101--117},
title = {{Evaluation and combination of pitch estimation methods for melody extraction in symphonic classical music}},
url = {https://repositori.upf.edu/bitstream/handle/10230/26985/Bosch{\_}NewMusic{\_}Eval.pdf?sequence=1{\&}isAllowed=y},
volume = {45},
year = {2016}
}
@article{Salamon2012a,
abstract = {We present a novel system for the automatic extraction of the main melody from polyphonic music recordings. Our approach is based on the creation and characterization of pitch contours, time continuous sequences of pitch candidates grouped using auditory streaming cues. We define a set of contour characteristics and show that by studying their distributions we can devise rules to distinguish between melodic and non-melodic contours. This leads to the development of new voicing detection, octave error minimization and melody selection techniques. A comparative evaluation of the proposed approach shows that it outperforms current state-of-the-art melody extraction systems in terms of overall accuracy. Further evaluation of the algorithm is provided in the form of a qualitative error analysis and the study of the effect of key parameters and algorithmic components on system performance. Finally, we conduct a glass ceiling analysis to study the current limitations of the method, and possible directions for future work are proposed.},
author = {Salamon, Justin and Gomez, Emilia},
doi = {10.1109/TASL.2012.2188515},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Salamon, Rocha, G{\'{o}}mez - 2012 - Musical genre classification using melody features extracted from polyphonic music signals.pdf:pdf},
isbn = {1795-990X 0355-3140},
issn = {15587916},
journal = {IEEE Transactions on Audio, Speech and Language Processing},
keywords = {Audio content description,multi-pitch estimation,music information retrieval,pitch contour,predominant melody estimation},
number = {6},
pages = {1759--1770},
title = {{Melody extraction from polyphonic music signals using pitch contour characteristics}},
volume = {20},
year = {2012}
}
@article{Dressler2016,
abstract = {This dissertation addresses the problem of melody detection in polyphonic musical audio. The proposed algorithm uses a bottom-up design consisting of five processing modules: the spectral analysis, a pitch determination algorithm, the tracking of tone objects, the tracking of musical voices including the identification of the melody voice, and MIDI note estimation. Each module leads to a more abstract representation of the audio data and hence to a reduction of information load, which allows a very efficient computation of the melody. Nonetheless, the dataflow is not strictly unidirectional: on several occasions, feedback from higher processing modules controls the processing of low-level modules. The spectral analysis is based on a technique for the efficient computation of shorttime Fourier spectra in different time-frequency resolutions, the so called multiresolution FFT, which provides the best frequency resolution in the lowest frequency band, and an increasing time resolution in higher frequency bands. The pitch determination algorithm (PDA) is based on the pair-wise analysis of spectral peaks. The idea of the technique lies in the identification of partials with successive (odd) harmonic numbers and the subsequent subharmonic summation. Additional ratings are introduced to avoid octave errors and to discriminate partials from different audio sources, exploiting clues like harmonicity, timbral smoothness, the appearance of intermediate spectral peaks, and harmonic number. Salient pitches chosen from the resulting pitch spectrogram denote adequate starting points for high-level tone objects. Although melody detection implies a strong focus on the predominant voice, the proposed tone processing module aims at extracting multiple fundamental frequencies (F0). Current state-of-the-art algorithms promote either the iterative detection of the predominant pitch and its subsequent deletion, or the joint pitch candidate estimation. In this thesis, a novel approach is presented that combines the iterative with the joint method: the long-term spectral envelope of existing tones is used to inhibit spectral peaks of the current analysis frame prior to the pitch estimation. This feedback from the tone processing module ensures that the PDA detects primarily the fundamental frequencies of new tones. At the same time, the joint evaluation of active tones tackles the problem of shared harmonics and helps to uncover octave errors. In order to identify the melody, the best succession of tones has to be chosen. This thesis describes an efficient computational method for auditory stream segregation that processes a variable number of simultaneous voices. Although no statistical model is implemented, probabilistic relationships that can be observed in melody tone sequences are exploited. The MIDI note module of the algorithm aims at the identification of the tone's onsets and offsets, as well as the assignment of a discrete tone height according to an equal temperament scale, which does not depend on a predetermined reference frequency. While the MIDI module uses at least some musicological knowledge, all other algorithm parts do not make prior assumptions about the instruments, the music genre, the tuning, or even musical scales. For this reason, the system can be utilized on different kinds of audio. The presented melody extraction algorithm has been evaluated during the MIREX audio melody extraction task. MIREX stands for Music Information Retrieval Evaluation eXchange. The goal of this exchange is to compare algorithms and systems relevant for the multidisciplinary field of Music Information Retrieval, including symbolic music, audio, and other subdisciplines. The MIREX results show that the proposed algorithm belongs to the state-of-the-art-algorithms, reaching the best overall accuracy in MIREX 2014. Moreover, MIREX results in the multiple F0 detection task show that the proposed tone processing method allows a reliable and very efficient identification of multiple F0s. Another strength of the melody extraction algorithm is its computational efficiency and its potential for real-time processing with a small time latency.},
author = {Dressler, Karin},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/4c947a49ad59de9e0bf7d37b158da82ca09efbf6.pdf:pdf},
keywords = {review},
title = {{Automatic Transcription of the Melody from Polyphonic Music}},
url = {https://www.db-thueringen.de/servlets/MCRFileNodeServlet/dbt{\_}derivate{\_}00038847/ilm1-2017000136.pdf},
year = {2016}
}
@article{Bittner2015,
author = {Bittner, Rachel M. and Salamon, Justin and Essid, Slim and Bello, Juan P.},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/03e68704aea0929e217e0f99449f458ba85d7c44.pdf:pdf},
isbn = {978-84-606-8853-2},
journal = {16th International Society for Music Information Retrieval Conference (ISMIR 2015)},
pages = {500--506},
title = {{Melody extraction by contour classification}},
url = {http://ismir2015.uma.es/articles/247{\_}Paper.pdf},
year = {2015}
}
@article{Bosch2016a,
abstract = {Thiswork explores the use of source-filter models for pitch salience estimation and their combination with different pitch tracking and voicing estimation methods for auto- matic melody extraction. Source-filter models are used to create a mid-level representation of pitch that implic- itly incorporates timbre information. The spectrogram of a musical audio signal is modelled as the sum of the lead- ing voice (produced by human voice or pitched musical in- struments) and accompaniment. The leading voice is then modelled with a Smoothed Instantaneous Mixture Model (SIMM) based on a source-filter model. The main advan- tage of such a pitch salience function is that it enhances the leading voice even without explicitly separating it from the rest of the signal. We show that this is beneficial for melody extraction, increasing pitch estimation accu- racy and reducing octave errors in comparison with simpler pitch salience functions. The adequate combination with voicing detection techniques based on pitch contour char- acterisation leads to significant improvements over state- of-the-art methods, for both vocal and instrumental music.},
author = {Bosch, Juan J and Bittner, Rachel M and Salamon, Justin and Gomez, Emilia},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/a4e111bf2901d617eb080bdb4251b40ef180072e.pdf:pdf},
journal = {Proc. 17th International Society for Music Information Retrieval Conference},
keywords = {review},
mendeley-tags = {review},
number = {1},
title = {{A comparison of melody extraction methods based on source-filter modelling}},
url = {https://pdfs.semanticscholar.org/a4e1/11bf2901d617eb080bdb4251b40ef180072e.pdf},
year = {2016}
}
@article{Stoller2019,
abstract = {Time-aligned lyrics can enrich the music listening experience by enabling karaoke, text-based song retrieval and intra-song navigation, and other applications. Compared to text-to-speech alignment, lyrics alignment remains highly challenging, despite many attempts to combine numerous sub-modules including vocal separation and detection in an effort to break down the problem. Furthermore, training required fine-grained annotations to be available in some form. Here, we present a novel system based on a modified Wave-U-Net architecture, which predicts character probabilities directly from raw audio using learnt multi-scale representations of the various signal components. There are no sub-modules whose interdependencies need to be optimized. Our training procedure is designed to work with weak, line-level annotations available in the real world. With a mean alignment error of 0.35s on a standard dataset our system outperforms the state-of-the-art by an order of magnitude.},
archivePrefix = {arXiv},
arxivId = {1902.06797},
author = {Stoller, Daniel and Durand, Simon and Ewert, Sebastian},
eprint = {1902.06797},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/34e076f70f7a49d6594525e2a7ccbb3b740db9e9.pdf:pdf},
title = {{End-to-end Lyrics Alignment for Polyphonic Music Using an Audio-to-Character Recognition Model}},
url = {http://arxiv.org/abs/1902.06797},
year = {2019}
}
@article{Balke2017,
abstract = {Retrieving short monophonic queries in music recordings is a challenging research problem in Music Information Retrieval (MIR). In jazz music, given a solo transcription, one retrieval task is to find the corresponding (potentially polyphonic) recording in a music collection. Many conventional systems approach such retrieval tasks by first extracting the predominant F0-trajectory from the recording, then quantizing the extracted trajectory to musical pitches and finally comparing the resulting pitch sequence to the monophonic query. In this paper, we introduce a data-driven approach that avoids the hard decisions involved in conventional approaches: Given pairs of time-frequency (TF) representations of full music recordings and TF representations of solo transcriptions, we use a DNN-based approach to learn a mapping for transforming a �polyphonic� TF representation into a �monophonic� TF representation. This transform can be considered as a kind of solo voice enhancement. We evaluate our approach within a jazz solo retrieval scenario and compare it to a state-of-the-art method for predominant melody extraction.},
author = {Balke, Stefan and Dittmar, Christian and Abesser, Jakob and Muller, Meinard},
doi = {10.1109/ICASSP.2017.7952145},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Balke et al. - 2017 - Data-driven solo voice enhancement for jazz music retrieval.pdf:pdf},
isbn = {9781509041176},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {Music Information Retrieval,Neural Networks,Query-by-Example},
pages = {196--200},
title = {{Data-driven solo voice enhancement for jazz music retrieval}},
url = {https://www.audiolabs-erlangen.de/content/05-fau/assistant/balke/01-publications/2017{\_}BalkeDAM{\_}SoloVoiceEnhancement{\_}ICASSP.pdf},
year = {2017}
}
@article{Humphrey2014a,
abstract = {The continued growth of MIR is motivating more com-plex annotation data, consisting of richer information, mul-tiple annotations for a given task, and multiple tasks for a given music signal. In this work, we propose JAMS, a JSON-based music annotation format capable of address-ing the evolving research requirements of the community, based on the three core principles of simplicity, structure and sustainability. It is designed to support existing data while encouraging the transition to more consistent, com-prehensive, well-documented annotations that are poised to be at the crux of future MIR research. Finally, we pro-vide a formal schema, software tools, and popular datasets in the proposed format to lower barriers to entry, and dis-cuss how now is a crucial time to make a concerted effort toward sustainable annotation standards.},
author = {Humphrey, Eric J and Salamon, Justin and Nieto, Oriol and Forsyth, Jon and Bittner, Rachel M and Bello, Juan P},
file = {:home/jirka/Sta{\v{z}}en{\'{e}}/humphrey{\_}jams{\_}ismir2014.pdf:pdf},
journal = {Proceedings of the 15th International Society for Music Information Retrieval Conference},
number = {September},
pages = {591--596},
title = {{JAMS: A JSON Annotated Music Specification for Reproducible MIR Research}},
year = {2014}
}
@article{Poliner,
abstract = {Melodies provide an important conceptual summarization of polyphonic audio. The extraction of melodic content has practical applications ranging from content-based au-dio retrieval to the analysis of musical structure. In con-trast to previous transcription systems based on a model of the harmonic (or periodic) structure of musical pitches, we present a classification-based system for performing au-tomatic melody transcription that makes no assumptions beyond what is learned from its training data. We evalu-ate the success of our algorithm by predicting the melody of the ISMIR 2004 Melody Competition evaluation set and on newly-generated test data. We show that a Sup-port Vector Machine melodic classifier produces results comparable to state of the art model-based transcription systems.},
author = {Poliner, Graham E and Ellis, Daniel P W},
file = {:home/jirka/Sta{\v{z}}en{\'{e}}/ismir05-melody.pdf:pdf},
isbn = {0955117909},
journal = {Proc. 6th Int. Conf. on Music Inform. Retrieval},
keywords = {a set of,a specific audio struc-,classification,fundamental,harmonics of a particular,melody transcription,namely that a musical,pitch is realized as,rule-based analysis that assumes,this assump-,ture},
number = {1},
pages = {161--166},
title = {{A Classification Approach to Melody Transcription}},
year = {2005}
}
@article{Oord2016,
abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
archivePrefix = {arXiv},
arxivId = {1609.03499},
author = {van den Oord, Aaron and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
eprint = {1609.03499},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/52eec5b914f72c4cd3f03eaedf1d38bb9a4df6de.pdf:pdf},
pages = {1--15},
title = {{WaveNet: A Generative Model for Raw Audio}},
url = {http://arxiv.org/abs/1609.03499},
year = {2016}
}
@article{Hsu2010,
abstract = {This paper proposes a novel and effective approach to extract the pitches of the singing voice from monaural polyphonic songs. The sinusoidal partials of the musical audio signals are first extracted. The Fourier transform is then applied to extract the vibrato/tremolo information of each partial. Some criteria based on this vibrato/tremolo information are employed to discriminate the vocal par- tials from the music accompaniment partials. Besides, a singing pitch trend estimation algorithm which is able to find the global singing progressing tunnel is also pro- posed. The singing pitches can then be extracted more robustly via these two processes. Quantitative evaluation shows that the proposed algorithms significantly improve the raw pitch accuracy of our previous approach and are comparable with other state of the art approaches submit- ted to MIREX.},
author = {Hsu, Chao-ling and Jang, Jyh-shing Roger},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/8b8fc6e3d4aaa1b424c6cb80fd0137f138b24af6.pdf:pdf},
isbn = {9789039353813},
journal = {11th Int. Soc. for Music Info. Retrieval Conf.},
number = {Ismir},
pages = {525--530},
title = {{Singing Pitch Extraction by Voice Vibrato/Tremolo Estimation and Instrument Partial Deletion}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=9BE69D930B1BEADFCD07E4534BA4E7E0?doi=10.1.1.231.4599{\&}rep=rep1{\&}type=pdf},
year = {2010}
}
@article{Su2018,
abstract = {A patch-based convolutional neural network (CNN) model presented in this paper for vocal melody extraction in polyphonic music is inspired from object detection in image processing. The input of the model is a novel time-frequency representation which enhances the pitch contours and suppresses the harmonic components of a signal. This succinct data representation and the patch-based CNN model enable an efficient training process with limited labeled data. Experiments on various datasets show excellent speed and competitive accuracy comparing to other deep learning approaches.},
archivePrefix = {arXiv},
arxivId = {arXiv:1804.09202v1},
author = {Su, Li},
doi = {10.1109/ICASSP.2018.8462420},
eprint = {arXiv:1804.09202v1},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/e041d53b47c0259a9ea025b50ff4a2e04972b8e4.pdf:pdf},
isbn = {9781538646588},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {Cepstrum,Convolutional neural networks,Melody extraction,Music information retrieval},
pages = {371--375},
title = {{Vocal melody extraction using patch-based CNN}},
url = {https://arxiv.org/pdf/1804.09202.pdf},
volume = {2018-April},
year = {2018}
}
@article{Kum2016,
abstract = {Singing melody extraction is a task that tracks pitch con- tour of singing voice in polyphonic music. While the ma- jority of melody extraction algorithms are based on com- puting a saliency function of pitch candidates or sepa- rating the melody source from the mixture, data-driven approaches based on classification have been rarely ex- plored. In this paper, we present a classification-based approach for melody extraction on vocal segments us- ing multi-column deep neural networks. In the proposed model, each of neural networks is trained to predict a pitch label of singing voice from spectrogram, but their outputs have different pitch resolutions. The final melody contour is inferred by combining the outputs of the networks and post-processing it with a hidden Markov model. In order to take advantage of the data-driven approach, we also aug- ment training data by pitch-shifting the audio content and modifying the pitch label accordingly. We use the RWC dataset and vocal tracks of the MedleyDB dataset for train- ing the model and evaluate it on the ADC 2004, MIREX 2005 and MIR-1k datasets. Through several settings of experiments, we show incremental improvements of the melody prediction. Lastly, we compare our best result to those of previous state-of-the-arts.},
author = {Kum, Sangeun and Oh, Changheun and Nam, Juhan},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2e705f145e3f6f184f52d26307180f0d5c24b7d2.pdf:pdf},
isbn = {978-0-692-75506-8},
journal = {Proceedings of the 17th International Society for Music Information Retrieval Conference, (ISMIR)},
number = {August},
pages = {819--825},
title = {{Melody Extraction on Vocal Segments Using Multi-Column Deep Neural Networks}},
url = {https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/119{\_}Paper.pdf},
year = {2016}
}
@article{Gabor1945,
author = {Gabor, By D and Member, Associate},
file = {:home/jirka/05298517.pdf:pdf},
journal = {Journal of the Institution of Electrical Engineers-Part III: Radio and Communication Engineering},
number = {1946},
pages = {429--441},
title = {{THEORY OF COMMUNICATION * Part 1 . THE ANALYSIS OF INFORMATION}},
volume = {93},
year = {1945}
}
@article{Cancela2010,
author = {Cancela, Pablo and L{\'{o}}pez, Ernesto and Rocamora, Mart{\'{i}}n},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/519a2e9c7d266e5cf6fb94f1617124f8c26e17f0.pdf:pdf},
journal = {Audio},
number = {1},
pages = {1--8},
title = {{FAN CHIRP TRANSFORM FOR MUSIC REPRESENTATION}},
url = {https://iie.fing.edu.uy/publicaciones/2010/CLR10/CLR10.pdf},
year = {2010}
}
@article{Martak2018,
abstract = {Deep neural networks - especially new, sophisticated architectures−are gaining increased competence at various tasks of growing complexity. Automatic transcription of polyphonic music is a complex problem with state-of-the-art approaches still suffering from high error rates relative to human-level performance. WaveNet is a novel deep neural network architec- ture with significant audio modelling capacity, operating directly on samples of digital audio signal. We propose an approach to polyphonic note transcription using WaveNet for feature extraction from raw audio and recognition of note presence. We show, that WaveNet can be used to detect pitch in polyphonic audio texture, as opposed to most other approaches, which mostly depend on time-frequency representations. Model outputs are smoothed and truncated using one of two alternative thresholding techniques. We evaluate our method on recently released dataset MusicNet and compare frame-level transcription performance to benchmark models. Results are promising, although further evaluation is needed to explore performance limits of this method.},
author = {Martak, Lukas S. and Sajgalik, Marius and Benesova, Wanda},
doi = {10.1109/IWSSIP.2018.8439708},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/62363e77d902b4124fa9ee0dc10df7cb74793814.pdf:pdf},
isbn = {9781538669792},
issn = {21578702},
journal = {International Conference on Systems, Signals, and Image Processing},
keywords = {Deep End-to-End Learning,Multi-Pitch Estimation,Neural Network,Polyphonic Note Transcription,WaveNet},
pages = {2--6},
title = {{Polyphonic Note Transcription of Time-Domain Audio Signal with Deep WaveNet Architecture}},
url = {https://vgg.fiit.stuba.sk/wp-uploads/2018/09/iwssip2018{\_}wavenet.pdf},
volume = {2018-June},
year = {2018}
}
@techreport{Bittner2018,
abstract = {Fundamental frequency (f0) estimation from polyphonic music includes the tasks of multiple-f0, melody, vocal, and bass line estimation. Historically these problems have been approached separately, and only recently, using learning-based approaches. We present a multitask deep learning architecture that jointly estimates outputs for various tasks including multiple-f0, melody, vocal and bass line estimation, and is trained using a large, semi-automatically annotated dataset. We show that the multitask model outperforms its single-task counterparts, and explore the effect of various design decisions in our approach, and show that it performs better or at least competitively when compared against strong baseline methods.},
annote = {- Paper ře{\v{s}}{\'{i}} nedostatek dat pomoc{\'{i}} multitask learningu, k různ{\'{y}}m MIR {\'{u}}loh{\'{a}}m jsou typicky různ{\'{e}} datasety},
archivePrefix = {arXiv},
arxivId = {1809.00381},
author = {Bittner, Rachel M. and McFee, Brian and Bello, Juan P.},
doi = {arXiv:1809.00381v1},
eprint = {1809.00381},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bittner, Mcfee, Bello - Unknown - Multitask Learning for Fundamental Frequency Estimation in Music.pdf:pdf},
keywords = {melody extraction,multiple f0 estimation},
mendeley-tags = {melody extraction,multiple f0 estimation},
title = {{Multitask Learning for Fundamental Frequency Estimation in Music}},
url = {http://arxiv.org/abs/1809.00381},
year = {2018}
}
@article{Rao2010,
abstract = {Melody extraction algorithms for single-channel polyphonic music typically rely on the salience of the lead melodic instrument, considered here to be the singing voice. However the simultaneous presence of one or more pitched instruments in the polyphony can cause such a predominant-F0 tracker to switch between tracking the pitch of the voice and that of an instrument of comparable strength, resulting in reduced voice-pitch detection accuracy. We propose a system that, in addition to biasing the salience measure in favor of singing voice characteristics, acknowledges that the voice may not dominate the polyphony at all instants and therefore tracks an additional pitch to better deal with the potential presence of locally dominant pitched accompaniment. A feature based on the temporal instability of voice harmonics is used to finally identify the voice pitch. The proposed system is evaluated on test data that is representative of polyphonic music with strong pitched accompaniment. Results show that the proposed system is indeed able to recover melodic information lost to its single-pitch tracking counterpart, and also outperforms another state-of-the-art melody extraction system designed for polyphonic music.},
author = {Rao, Vishweshwara and Rao, Preeti},
doi = {10.1109/TASL.2010.2042124},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/e60f54210c8aac9f1bcd58be48f73e7a698bd844.pdf:pdf},
issn = {15587916},
journal = {IEEE Transactions on Audio, Speech and Language Processing},
keywords = {Fundamental frequency estimation,Music information retrieval (MIR),Music transcription,Predominant pitch detection},
number = {8},
pages = {2145--2154},
title = {{Vocal melody extraction in the presence of pitched accompaniment in polyphonic music}},
url = {https://www.ee.iitb.ac.in/course/{~}daplab/publications/PrePrintForWebsite.pdf},
volume = {18},
year = {2010}
}
@article{Salamon2014,
abstract = {Melody extraction algorithms aim to produce a sequence of frequency values corresponding to the pitch of the dominant melody from a musical recording. Over the past decade, melody extraction has emerged as an active research topic, comprising a large variety of proposed algorithms spanning a wide range of techniques. This article provides an overview of these techniques, the applications for which melody extraction is useful, and the challenges that remain. We start with a discussion of ?melody? from both musical and signal processing perspectives and provide a case study that interprets the output of a melody extraction algorithm for specific excerpts. We then provide a comprehensive comparative analysis of melody extraction algorithms based on the results of an international evaluation campaign. We discuss issues of algorithm design, evaluation, and applications that build upon melody extraction. Finally, we discuss some of the remaining challenges in melody extraction research in terms of algorithmic performance, development, and evaluation methodology.},
author = {Salamon, Justin and G{\'{o}}mez, Emilia and Ellis, Daniel P.W. and Richard, Ga{\"{e}}l},
doi = {10.1109/MSP.2013.2271648},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/5b3f05ede781769754d9bcb7349d33447716d75d.pdf:pdf},
isbn = {1053-5888},
issn = {10535888},
journal = {IEEE Signal Processing Magazine},
keywords = {review},
mendeley-tags = {review},
number = {2},
pages = {118--134},
pmid = {24740412},
title = {{Melody extraction from polyphonic music signals: Approaches, applications, and challenges}},
url = {http://www.justinsalamon.com/uploads/4/3/9/4/4394963/salamon{\_}gomez{\_}ellis{\_}richard{\_}melodyextractionreview{\_}ieeespm{\_}2013.pdf},
volume = {31},
year = {2014}
}
@article{Li2019,
abstract = {We introduce a dataset for facilitating audio-visual analysis of music performances. The dataset comprises 44 simple multi-instrument classical music pieces assembled from coordinated but separately recorded performances of individual tracks. For each piece, we provide the musical score in MIDI format, the audio recordings of the individual tracks, the audio and video recording of the assembled mixture, and ground-truth annotation files including frame-level and note-level transcriptions. We describe our methodology for the creation of the dataset, particularly highlighting our approaches for addressing the challenges involved in maintaining synchronization and expressiveness. We demonstrate the high quality of synchronization achieved with our proposed approach by comparing the dataset with existing widely-used music audio datasets. We anticipate that the dataset will be useful for the development and evaluation of existing music information retrieval (MIR) tasks, as well as for novel multi-modal tasks. We benchmark two existing MIR tasks (multi-pitch analysis and score-informed source separation) on the dataset and compare with other existing music audio datasets. Additionally, we consider two novel multi-modal MIR tasks (visually informed multi-pitch analysis and polyphonic vibrato analysis) enabled by the dataset and provide evaluation measures and baseline systems for future comparisons (from our recent work). Finally, we propose several emerging research directions that the dataset enables.},
archivePrefix = {arXiv},
arxivId = {arXiv:1612.08727v3},
author = {Li, Bochen and Liu, Xinzhao and Dinesh, Karthik and Duan, Zhiyao and Sharma, Gaurav},
doi = {10.1109/TMM.2018.2856090},
eprint = {arXiv:1612.08727v3},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/f71cc74459b40074a8d599386d65d6a13073bda0.pdf:pdf},
issn = {15209210},
journal = {IEEE Transactions on Multimedia},
keywords = {Multimodal music dataset,audio-visual analysis,dataset,music performance,synchronization},
mendeley-tags = {dataset},
number = {2},
pages = {522--535},
title = {{Creating a Multitrack Classical Music Performance Dataset for Multimodal Music Analysis: Challenges, Insights, and Applications}},
url = {https://arxiv.org/pdf/1612.08727.pdf},
volume = {21},
year = {2019}
}
@inproceedings{balke2016towards,
author = {Balke, Stefan and Driedger, Jonathan and Abe{\ss}er, Jakob and Dittmar, Christian},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mills - 1987 - Coyote spatial use in relation to prey abundance.pdf:pdf},
title = {{Towards Evaluating Multiple Predominant Melody Annotations in Jazz Recordings.}},
year = {2016}
}
@article{Bittner2018a,
author = {Bittner, Rachel M.},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bittner - 2018 - Data-Driven Fundamental Frequency Estimation.pdf:pdf},
keywords = {review},
mendeley-tags = {review},
title = {{Data-Driven Fundamental Frequency Estimation}},
year = {2018}
}
@article{Paiva2004,
abstract = {We present a method for melody detection in polyphonic musical signals based on a model of the human auditory system. First, a set of pitch candidates is obtained for each frame, based on the output of an ear model and periodicity detection using correlograms. Trajectories of the most salient pitches are then constructed. Next, note candidates are obtained by trajectory segmentation (in terms of frequency and pitch salience variations). Too short, low-salience and harmonically-related notes are then eliminated. Finally, the melody is extracted by selecting the most important notes at each time, based on their pitch salience. We tested our method with excerpts from 12 songs en- compassing several genres. In the songs where the solo stands out clearly, most of the melody notes were successfully detected. However, for songs where the melody is not that salient, the algorithm was not very accurate. Nevertheless, the followed approach seems promising.},
author = {Paiva, Rui Pedro and Mendes, Teresa and Cardoso, Am{\'{i}}lcar},
doi = {10.1007/978-3-540-31807-1_2},
file = {:home/jirka/Sta{\v{z}}en{\'{e}}/An{\_}Auditory{\_}Model{\_}Based{\_}Approach{\_}for{\_}Melody{\_}Detect.pdf:pdf},
isbn = {9783540318071},
number = {May 2014},
pages = {21--40},
title = {{An Auditory Model Based Approach for Melody Detection in Polyphonic Musical Recordings}},
year = {2004}
}
@article{Yeh2012,
abstract = {In this paper, we propose a hybrid method for singing pitch extraction from polyphonic audio music. We have observed several kinds of pitch errors made by a previously proposed algorithm based on trend estimation. We also noticed that other pitch tracking methods tend to have other types of pitch error. Then it becomes intuitive to combine the results of several pitch trackers to achieve a better accuracy. In this paper, we adopt 3 methods as a committee to determine the pitch, including the trend-estimation-based method for forward and backward signals, and training-based HMM method. Experimental results demonstrate that the proposed approach outperforms the best algorithm for the task of audio melody extraction in MIREX 2010.},
author = {Yeh, Tzu Chun and Wu, Ming Ju and Jang, Jyh Shing Roger and Chang, Wei Lun and Liao, I. Bin},
doi = {10.1109/ICASSP.2012.6287915},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/b6f5e4de4dbf20b3416f23d85d0ea119f56415b1.pdf:pdf},
isbn = {9781467300469},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {Audio Melody Extraction,Hidden Markov Model,Singing Pitch Extraction,Trend Estimation},
number = {August},
pages = {457--460},
title = {{A hybrid approach to singing pitch extraction based on trend estimation and hidden Markov models}},
url = {https://www.researchgate.net/profile/Jyh-Shing{\_}Jang/publication/261113746{\_}A{\_}hybrid{\_}approach{\_}to{\_}singing{\_}pitch{\_}extraction{\_}based{\_}on{\_}trend{\_}estimation{\_}and{\_}hidden{\_}Markov{\_}models/links/55d5446f08ae6788fa352c8e/A-hybrid-approach-to-singing-pitch-extraction-based-o},
year = {2012}
}
@article{DBasaranSEssid2018,
abstract = {Estimating the main melody of a polyphonic audio recording remains a challenging task. We approach the task from a classification perspective and adopt a convolutional recurrent neural network (CRNN) architecture that relies on a particular form of pretraining by source-filter nonneg-ative matrix factorisation (NMF). The source-filter NMF decomposition is chosen for its ability to capture the pitch and timbre content of the leading voice/instrument, providing a better initial pitch salience than standard time-frequency representations. Starting from such a musically motivated representation, we propose to further enhance the NMF-based salience representations with CNN layers , then to model the temporal structure by an RNN network and to estimate the dominant melody with a final classification layer. The results show that such a system achieves state-of-the-art performance on the MedleyDB dataset without any augmentation methods or large training sets.},
annote = {K{\'{o}}d:
https://github.com/dogacbasaran/ismir2018{\_}dominant{\_}melody{\_}estimation},
author = {{D Basaran, S Essid}, G Peeters},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/D Basaran, S Essid - 2018 - Main melody extraction with source-filter nmf and crnn.pdf:pdf},
journal = {Ismir},
pages = {82--89},
title = {{Main melody extraction with source-filter nmf and crnn}},
url = {http://ismir2018.ircam.fr/doc/pdfs/273{\_}Paper.pdf},
year = {2018}
}
@article{Bittner2014,
abstract = {We introduce MedleyDB: a dataset of annotated, royalty- free multitrack recordings. The dataset was primarily de- veloped to support research on melody extraction, address- ing important shortcomings of existing collections. For each song we provide melody f0 annotations as well as instrument activations for evaluating automatic instrument recognition. The dataset is also useful for research on tasks that require access to the individual tracks of a song such as source separation and automatic mixing. In this paper we provide a detailed description of MedleyDB, including curation, annotation, and musical content. To gain insight into the new challenges presented by the dataset, we run a set of experiments using a state-of-the-art melody extrac- tion algorithm and discuss the results. The dataset is shown to be considerably more challenging than the current test sets used in the MIREX evaluation campaign, thus open- ing new research avenues in melody extraction research. 1.},
author = {Bittner, Rachel and Salamon, Justin and Tierney, Mike and Mauch, Matthias and Cannam, Chris and Bello, Juan},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bittner et al. - 2014 - MedleyDB A multitrack dataset for annotation - intensive mir research.pdf:pdf},
journal = {International Society for Music Information Retrieval Conference},
keywords = {dataset},
mendeley-tags = {dataset},
title = {{MedleyDB: A multitrack dataset for annotation - intensive mir research}},
year = {2014}
}
@article{Su2016,
abstract = {While recent years have witnessed large progress in the al-gorithm of automatic music transcription (AMT), the development of general and sizable datasets for AMT evaluation is relatively stagnant, predominantly due to the fact that manually annotating and checking such datasets is labor-intensive and time-consuming. In this paper we propose a novel note-level annotation method for building AMT datasets by utilizing human's ability in following music in real-time. To test the quality of the annotation, we further propose an efficient method in qual-ifying an AMT dataset based on the concepts of onset error difference and the tolerance computed from the evaluation result. According to the experiments on five piano solos and four woodwind quintets, we claim that the proposed annotation method is reliable for evaluation of AMT algorithms.},
author = {Su, Li and Yang, Yi Hsuan},
doi = {10.1007/978-3-319-46282-0_20},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/9f7c676404f99816d5b06e29076bebb7879047e5.pdf:pdf},
isbn = {9783319462813},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Automatic music transcription,Multipitch estimation,Note tracking,Onset error difference,dataset},
mendeley-tags = {dataset},
pages = {309--321},
title = {{Escaping from the abyss of manual annotation: New methodology of building polyphonic datasets for automatic music transcription}},
url = {http://mac.citi.sinica.edu.tw/{~}yang/pub/su15cmmr.pdf},
volume = {9617 LNCS},
year = {2016}
}
@article{Raffel2014,
abstract = {Central to the field of MIR research is the evaluation of algorithms used to extract information from music data. We present mir{\_}eval, an open source software library which provides a transparent and easy-to-use implementation of the most common metrics used to measure the performance of MIR algorithms. In this paper, we enumerate the metrics implemented by mir{\_}eval and quantitatively compare each to existing implementations. When the scores reported by mir{\_}eval differ substantially from the reference, we detail the differences in implementation. We also provide a brief overview of mir{\_}eval's architecture, design, and intended use.},
author = {Raffel, Colin and Mcfee, Brian and Humphrey, Eric J. and Salamon, Justin and Nieto, Oriol and Liang, Dawen and Ellis, Daniel P. W.},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/18744f1ff827685c6d8d2919aef8b3bd32291fcd.pdf:pdf},
journal = {Proceedings of the 15th International Society for Music Information Retrieval Conference (ISMIR 2014)},
pages = {367--372},
title = {{mir$\backslash${\_}eval: A Transparent Implementation of Common MIR Metrics}},
url = {https://bmcfee.github.io/papers/ismir2014{\_}mireval.pdf},
year = {2014}
}
@book{Pfleiderer,
author = {Pfleiderer, Martin and Frieler, Klaus and Abe{\ss}er, Jakob and Zaddach, Wolf-Georg and Burkhart, Benjamin},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/dfbe6f589b03458850e7e510bb316257ea9527e2.pdf:pdf},
isbn = {9783959831246},
title = {{Inside the Jazzomat}},
url = {http://schott-campus.com/wp-content/uploads/2017/11/inside{\_}the{\_}jazzomat{\_}final{\_}rev{\_}oa4.pdf}
}
@article{Arora2013,
abstract = {Extraction of predominant melody from the musical performances containing various instruments is one of the most challenging task in the field of music information retrieval and computational musicology. This paper presents a novel framework which estimates predominant vocal melody in real-time by tracking various sources with the help of harmonic clusters (combs) and then determining the predominant vocal source by using the harmonic strength of the source. The novel on-line harmonic comb tracking approach complies with both structural as well as temporal constraints simultaneously. It relies upon the strong higher harmonics for robustness against distortion of the first harmonic due to low frequency accompaniments, in contrast to the existing methods which track the pitch values. The predominant vocal source identification depends upon the novel idea of source dependant filtering of recognition score, which allows the algorithm to be implemented on-line. The proposed method, although on-line, is shown to significantly outperform our implementation of a state-of-the-art offline method for vocal melody extraction. Evaluations also show the reduction in octave error and the effectiveness of novel score filtering technique in enhancing the performance. {\textcopyright} 2006-2012 IEEE.},
author = {Arora, Vipul and Behera, Laxmidhar},
doi = {10.1109/TASL.2012.2227731},
file = {:home/jirka/Sta{\v{z}}en{\'{e}}/06353544.pdf:pdf},
issn = {15587916},
journal = {IEEE Transactions on Audio, Speech and Language Processing},
keywords = {Music information retrieval,pitch tracking,spectral Harmonics,vocal melody estimation},
number = {3},
pages = {520--530},
publisher = {IEEE},
title = {{On-line melody extraction from polyphonic audio using harmonic cluster tracking}},
volume = {21},
year = {2013}
}
@article{Marolt2004,
abstract = {The paper presents our approach to the problem of finding melodic line(s) in polyphonic audio recordings. The approach is composed of two different stages, partially rooted in psychoacoustic theories of music perception: the first stage is dedicated to finding regions with strong and stable pitch (melodic fragments), while in the sec- ond stage, these fragments are grouped according to their properties (pitch, loudness...) into clusters which represent melodic lines of the piece. Expectation Maximization algorithm is used in both stages to find the dominant pitch in a region, and to train Gaussian Mixture Models that group fragments into melodies. The paper presents the entire process in more detail and provides some initial results.},
author = {Marolt, Matija},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/3301149ba2f5440dd4f7dca262f06b337a3c823f.pdf:pdf},
journal = {Proc. of the 7th Int. Conference on Digital Audio Effects},
pages = {5--9},
title = {{On finding melodic lines in audio recordings}},
url = {https://ant-s4.unibw-hamburg.de/dafx/paper-archive/2004/P{\_}217.PDF},
year = {2004}
}
@article{Stoller2018a,
abstract = {A main challenge in applying deep learning to music processing is the availability of training data. One potential solution is Multi-task Learning, in which the model also learns to solve related auxiliary tasks on additional datasets to exploit their correlation. While intuitive in principle, it can be challenging to identify related tasks and construct the model to optimally share information between tasks. In this paper, we explore vocal activity detection as an additional task to stabilise and improve the performance of vocal separation. Further, we identify problematic biases specific to each dataset that could limit the generalisation capability of separation and detection models, to which our proposed approach is robust. Experiments show improved performance in separation as well as vocal detection compared to single-task baselines. However, we find that the commonly used Signal-to-Distortion Ratio (SDR) metrics did not capture the improvement on non-vocal sections, indicating the need for improved evaluation methodologies.},
archivePrefix = {arXiv},
arxivId = {arXiv:1804.01650v1},
author = {Stoller, Daniel and Ewert, Sebastian and Dixon, Simon},
doi = {10.1007/978-3-319-93764-9_31},
eprint = {arXiv:1804.01650v1},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/b557889e5063e8a6dc69ca7d9d71643a255066f2.pdf:pdf},
isbn = {9783319937632},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Multi-task learning,Singing voice separation,Vocal activity detection,multitask},
mendeley-tags = {multitask},
pages = {329--339},
title = {{Jointly detecting and separating singing voice: A multi-task approach}},
url = {https://arxiv.org/pdf/1804.01650.pdf},
volume = {10891 LNCS},
year = {2018}
}
@article{Rigaud2016,
author = {Rigaud, Francois and Radenen, Mathieu},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/027ecd18db48bc49d4211f6281b321b9b34dfa56.pdf:pdf},
journal = {Ismir},
pages = {737--743},
title = {{Singing Voice Melody Transcription Using Deep Neural Networks}},
url = {https://s18798.pcdn.co/ismir2016/wp-content/uploads/sites/2294/2016/07/163{\_}Paper.pdf},
year = {2016}
}
@article{Ryynanen2008,
abstract = {First, it provides an easy way of obtaining a of a recording, allowing musicians to play it. the produced transcriptions may be used in mu- sic , information retrieval (MIR) from large databases, content-based audio processing, and},
author = {Ryyn{\"{a}}nen, Matti P. and Klapuri, Anssi P.},
doi = {10.1162/comj.2008.32.3.72},
file = {:home/jirka/40072648.pdf:pdf},
issn = {0148-9267},
journal = {Computer Music Journal},
number = {3},
pages = {72--86},
title = {{Automatic Transcription of Melody, Bass Line, and Chords in Polyphonic Music}},
volume = {32},
year = {2008}
}
@article{Hawthorne2018a,
abstract = {Generating musical audio directly with neural networks is notoriously difficult because it requires coherently modeling structure at many different timescales. Fortunately, most music is also highly structured and can be represented as discrete note events played on musical instruments. Herein, we show that by using notes as an intermediate representation, we can train a suite of models capable of transcribing, composing, and synthesizing audio waveforms with coherent musical structure on timescales spanning six orders of magnitude ({\~{}}0.1 ms to {\~{}}100 s), a process we call Wave2Midi2Wave. This large advance in the state of the art is enabled by our release of the new MAESTRO (MIDI and Audio Edited for Synchronous TRacks and Organization) dataset, composed of over 172 hours of virtuosic piano performances captured with fine alignment ({\~{}}3 ms) between note labels and audio waveforms. The networks and the dataset together present a promising approach toward creating new expressive and interpretable neural models of music.},
archivePrefix = {arXiv},
arxivId = {1810.12247},
author = {Hawthorne, Curtis and Stasyuk, Andriy and Roberts, Adam and Simon, Ian and Huang, Cheng-Zhi Anna and Dieleman, Sander and Elsen, Erich and Engel, Jesse and Eck, Douglas},
eprint = {1810.12247},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/730aeb1a46144b4824d2906a4c5d669ec589fa86.pdf:pdf},
pages = {1--12},
title = {{Enabling Factorized Piano Music Modeling and Generation with the MAESTRO Dataset}},
url = {http://arxiv.org/abs/1810.12247},
year = {2018}
}
@article{Devaney2007,
abstract = {Background in music theory and analysis. Polyphonic vocal intonation practices have been addressed in a number of studies on vocal acoustics. Our research both builds on this work and supplements it with a theoretical paradigm based on work done in the areas of sensory cononance and tonal attraction. Background in computing. Recent work in the field of music information retrieval has discussed the main obstacles related to tracking pitches in a polyphonic signal and has provided some techniques for working around these problems. Our method for analyzing the pitch content of recorded performances draws extensively on this work and on the knowledge made available to us by the musical scores of the pieces being performed. Aims. Our research is focused on the study and modeling of polyphonic vocal intonation practices through the intersection of computational and theoretical approaches. We present a methodology that allows for a detailed model of this aspect of polyphonic vocal performance practice to be built from analyses of numerous recordings of real-world performances, while working within a robust theoretical paradigm. Main contribution. In the computational component of the research a number of a cappella polyphonic vocal recordings are analyzed with signal processing techniques to estimate the perceived fundamental frequencies for the sung notes. These observations can be related to the musical context of the score through machine learning techniques to determine likely intonation tendencies for regularly occurring musical patterns. A major issue in developing a theory of intonation practices is the potential conflict between the vertical and horizontal intonational impetuses. To assess this conflict in greater detail we have constructed a theoretical approach where theories of sensory consonance account for vertical tuning tendencies and theories of tonal attraction account for the horizontal tendencies. Implications. In the field of music cognition, our research relates to work being done in the area of musical expression. If the intonation tendencies inferred from the end results of this research are taken as a norm, then deviations from this norm, when these deviations are musically appropriate, can be viewed as expressive phenomena. Computer software implementing such results will allow composers and musicologists to hear more intonationally accurate digital re-creations and may also function as a training guide for vocalists.},
author = {Devaney, Johanna and Ellis, Daniel},
issn = {13070401},
journal = {Journal of Interdisciplinary Music Studies [electronic version]},
keywords = {Intonation,machine learning,melodic attraction,sensory consonance,signal processing,singing},
title = {{An empirical approach to studying intonation tendencies in choral performances}},
year = {2007}
}
@article{Poliner2007,
abstract = {Although the process of analyzing an audio recording of a music performance is complex and difficult even for a human listener, there are limited forms of information that may be tractably extracted and yet still enable interesting applications. We discuss melody-roughly, the part a listener might whistle or hum-as one such reduced descriptor of music audio, and consider how to define it, and what use it might be. We go on to describe the results of full-scale evaluations of melody transcription systems conducted in 2004 and 2005, including an overview of the systems submitted, details of how the evaluations were conducted, and a discussion of the results. For our definition of melody, current systems can achieve around 70{\%} correct transcription at the frame level, including distinguishing between the presence or absence of the melody. Melodies transcribed at this level are readily recognizable, and show promise for practical applications {\textcopyright} 2007 IEEE.},
author = {Poliner, Graham E. and Ellis, Daniel P W and Ehmann, Andreas F. and G{\'{o}}mez, Emilia and Streich, Sebastian and Ong, Beesuan},
doi = {10.1109/TASL.2006.889797},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/249cccf6650cc19f43e8b62b63a4c6268736d9f8.pdf:pdf},
issn = {15587916},
journal = {IEEE Transactions on Audio, Speech and Language Processing},
keywords = {Audio,Evaluation,Melody transcription,Music,review},
mendeley-tags = {review},
number = {4},
pages = {1247--1256},
title = {{Melody transcription from music audio: Approaches and evaluation}},
url = {https://academiccommons.columbia.edu/doi/10.7916/D8NC69RK/download},
volume = {15},
year = {2007}
}
@article{Mauch2014a,
abstract = {We propose the Probabilistic YIN (PYIN) algorithm, a modification of the well-known YIN algorithm for fundamental frequency (F0) estimation. Conventional YIN is a simple yet effective method for frame-wise monophonic F0 estimation and remains one of the most popular methods in this domain. In order to eliminate short-term errors, outputs of frequency estimators are usually post-processed resulting in a smoother pitch track. One shortcoming of YIN is that such post-processing cannot fall back on alternative interpretations of the signal because the method outputs precisely one estimate per frame. To address this problem we modify YIN to output multiple pitch candidates with associated probabilities (PYIN Stage 1). These probabilities arise naturally from a prior distribution on the YIN threshold parameter. We use these probabilities as observations in a hidden Markov model, which is Viterbi-decoded to produce an improved pitch track (PYIN Stage 2). We demonstrate that the combination of Stages 1 and 2 raises recall and precision substantially. The additional computational complexity of PYIN over YIN is low. We make the method freely available online1 as an open source C++ library for Vamp hosts. {\textcopyright} 2014 IEEE.},
author = {Mauch, Matthias and Dixon, Simon},
doi = {10.1109/ICASSP.2014.6853678},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mauch, Dixon - 2014 - PYIN A fundamental frequency estimator using probabilistic threshold distributions.pdf:pdf},
isbn = {9781479928927},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {Pitch estimation,YIN,pitch tracking},
number = {1},
pages = {659--663},
title = {{PYIN: A fundamental frequency estimator using probabilistic threshold distributions}},
url = {https://www.eecs.qmul.ac.uk/{~}simond/pub/2014/MauchDixon-PYIN-ICASSP2014.pdf},
volume = {1},
year = {2014}
}
@article{Downie2010,
author = {Downie, J Stephen and Ehmann, Andreas F and Bay, Mert and Jones, M Cameron},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/383615607f345a8cfbc5e884eaa5ca04f3c4b139.pdf:pdf},
journal = {Advances in Music Information Retrieval},
keywords = {evaluation,mirex,music information retrieval},
pages = {93--115},
title = {{eXchange : Some Observations and Insights}},
url = {https://pdfs.semanticscholar.org/3836/15607f345a8cfbc5e884eaa5ca04f3c4b139.pdf},
year = {2010}
}
@article{Paiva2006,
abstract = {Computer Music J., vol. 30, pp. 80–98, Dec. 2006.},
author = {Paiva, Rui Pedro and Mendes, Teresa and Cardoso, Am{\'{i}}lcar},
doi = {10.1162/comj.2006.30.4.80},
file = {:home/jirka/Sta{\v{z}}en{\'{e}}/CMJ2006.pdf:pdf},
issn = {0148-9267},
journal = {Computer Music Journal},
number = {4},
pages = {80--98},
title = {{Melody Detection in Polyphonic Musical Signals: Exploiting Perceptual Rules, Note Salience, and Melodic Smoothness}},
volume = {30},
year = {2006}
}
@article{Brown1990,
abstract = {The frequencies that have been chosen to make up the scale of Western music are geometrically spaced. Thus the discrete Fourier transform (DFT), although extremely efficient in the fast Fourier transform implementation, yields components which do not map ... $\backslash$n},
author = {Brown, Judith C.},
doi = {10.1121/1.400476},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/a667221790229863b9778688969a2544508cdfeb.pdf:pdf},
isbn = {0104251050},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
number = {1},
pages = {425--434},
title = {{Calculation of a constant Q spectral transform}},
url = {http://academics.wellesley.edu/Physics/brown/pubs/cq1stPaper.pdf},
volume = {89},
year = {1990}
}
@article{Bosch2016b,
abstract = {This work proposes a melody extraction method which combines a pitch salience function based on source-filter modelling with melody tracking based on pitch contour selection. We model the spectrogram of a musical au-dio signal as the sum of the leading voice and accompa-niment. The leading voice is modelled with a Smoothed Instantaneous Mixture Model (SIMM), and the accompa-niment is modelled with a Non-negative Matrix Factor-ization (NMF). The main benefit of this representation is that it incorporates timbre information, and that the lead-ing voice is enhanced, even without an explicit separation from the rest of the signal. Two different salience functions based on SIMM are proposed, in order to adapt the output of such model to the pitch contour based tracking. Can-didate melody pitch contours are then created by grouping pitch sequences, using auditory streaming cues. Finally, melody pitch contours are selected using a set of heuristic rules based on contour characteristics and smoothness con-straints. The evaluation on a large set of challenging poly-phonic music material, shows that the proposed salience functions help increasing the salience of melody pitches in comparison to similar methods. The complete melody extraction methods also achieve a higher overall accuracy than state-of-the-art approaches when evaluated on both vocal and instrumental music.},
author = {Bosch, Juan J and G{\'{o}}mez, Emilia},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bosch, G{\'{o}}mez - 2016 - Melody Extraction Based on a Source-Filter Model Using Pitch Contour Selection.pdf:pdf},
journal = {13th Conference on Sound and Music Computing},
pages = {67--74},
title = {{Melody Extraction Based on a Source-Filter Model Using Pitch Contour Selection}},
year = {2016}
}
@article{Park2019,
abstract = {We present SpecAugment, a simple data augmentation method for speech recognition. SpecAugment is applied directly to the feature inputs of a neural network (i.e., filter bank coefficients). The augmentation policy consists of warping the features, masking blocks of frequency channels, and masking blocks of time steps. We apply SpecAugment on Listen, Attend and Spell networks for end-to-end speech recognition tasks. We achieve state-of-the-art performance on the LibriSpeech 960h and Swichboard 300h tasks, outperforming all prior work. On LibriSpeech, we achieve 6.8{\%} WER on test-other without the use of a language model, and 5.8{\%} WER with shallow fusion with a language model. This compares to the previous state-of-the-art hybrid system of 7.5{\%} WER. For Switchboard, we achieve 7.2{\%}/14.6{\%} on the Switchboard/CallHome portion of the Hub5'00 test set without the use of a language model, and 6.8{\%}/14.1{\%} with shallow fusion, which compares to the previous state-of-the-art hybrid system at 8.3{\%}/17.3{\%} WER.},
archivePrefix = {arXiv},
arxivId = {1904.08779},
author = {Park, Daniel S. and Chan, William and Zhang, Yu and Chiu, Chung-Cheng and Zoph, Barret and Cubuk, Ekin D. and Le, Quoc V.},
eprint = {1904.08779},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/1042713dacdeea908dcb69fc44061cb0a883710f.pdf:pdf},
title = {{SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition}},
url = {http://arxiv.org/abs/1904.08779},
year = {2019}
}
@article{Salamon2012,
abstract = {We present a new method for musical genre classification based on high-level melodic features that are extracted directly from the audio signal of polyphonic music. The features are obtained through the automatic characterisation of pitch contours describing the predominant melodic line, extracted using a state-of-the-art audio melody extraction algorithm. Using standard machine learning algorithms the melodic features are used to classify excerpts into five different musical genres. We obtain a classification accuracy above 90{\{}{\%}{\}} for a collection of 500 excerpts, demonstrating that successful classi- fication can be achieved using high-level melodic features that are more meaningful to humans compared to low-level features commonly used for this task. We also compare our method to a base- line approach using low-level timbre features, and study the effect of combining these low-level features with our high-level melodic features. The results demonstrate that complementing low-level features with high-level melodic features is a promising approach.},
author = {Salamon, Justin and Rocha, Bruno and Gomez, Emilia},
doi = {10.1109/ICASSP.2012.6287822},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/a930e9b6124da5f1383b5d48bebd4c85717e5d09.pdf:pdf},
isbn = {9781467300469},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {Genre classification,melody extraction,pitch contour},
pages = {81--84},
title = {{Musical genre classification using melody features extracted from polyphonic music signals}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.296.5021{\&}rep=rep1{\&}type=pdf},
year = {2012}
}
@article{Duan,
author = {Duan, Zhiyao and Pardo, Bryan},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Duan, Pardo - Unknown - Bach10 Dataset ----A Versatile Polyphonic Music Dataset.pdf:pdf},
keywords = {dataset},
mendeley-tags = {dataset},
title = {{Bach10 Dataset ----A Versatile Polyphonic Music Dataset}}
}
@article{Engel2017,
abstract = {Generative models in vision have seen rapid progress due to algorithmic improvements and the availability of high-quality image datasets. In this paper, we offer contributions in both these areas to enable similar progress in audio modeling. First, we detail a powerful new WaveNet-style autoencoder model that conditions an autoregressive decoder on temporal codes learned from the raw audio waveform. Second, we introduce NSynth, a large-scale and high-quality dataset of musical notes that is an order of magnitude larger than comparable public datasets. Using NSynth, we demonstrate improved qualitative and quantitative performance of the WaveNet autoencoder over a well-tuned spectral autoencoder baseline. Finally, we show that the model learns a manifold of embeddings that allows for morphing between instruments, meaningfully interpolating in timbre to create new types of sounds that are realistic and expressive.},
archivePrefix = {arXiv},
arxivId = {1704.01279},
author = {Engel, Jesse and Resnick, Cinjon and Roberts, Adam and Dieleman, Sander and Eck, Douglas and Simonyan, Karen and Norouzi, Mohammad},
eprint = {1704.01279},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/595a23ab15bf32983b1d233ceca47b2146fb64e6.pdf:pdf},
title = {{Neural Audio Synthesis of Musical Notes with WaveNet Autoencoders}},
url = {http://arxiv.org/abs/1704.01279},
year = {2017}
}
@article{Goto2002,
abstract = {This paper describes the design policy and specifications of the RWC Music Database, a music database (DB) that is available to researchers for common use and research purposes. Various commonly available DBs have been built in other research fields and have made a significant contribution to the research in those fields. The field of musical information processing, however, has lacked a commonly available music DB. We therefore built the RWC Music Database which contains four original DBs: the Popular Music Database (100 pieces), Royalty-Free Music Database (15 pieces), Classical Music Database (50 pieces), and Jazz Music Database (50 pieces). Each consists of originally-recorded music compact discs, standard MIDI files, and text files of lyrics. These DBs are now available in Japan at a cost equal to only duplication, shipping, and handling charges (virtually for free), and we plan to make them available outside Japan. We hope that our DB will encourage further advances in musical information processing research.},
author = {Goto, Masataka and Hashiguchi, Hiroki and Nishimura, Takuichi and Oka, Ryuichi},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/de31aa8e914c60189a425e174af223967e2722cc.pdf:pdf},
journal = {International Conference on Music Information Retrieval},
number = {October},
pages = {287--288},
title = {{RWC Music Database: Popular, Classical, and Jazz Music Databases}},
url = {https://staff.aist.go.jp/m.goto/PAPER/ISMIR2002goto.pdf},
year = {2002}
}
@article{Tse2016,
abstract = {Music transcription is a highly complex task that is difficult for automated algorithms, and equally challenging to people, even those with many years of musical training. Furthermore, there is a shortage of high-quality datasets for training automated transcription algorithms. In this research, we explore a semi-automated, crowdsourced approach to generate music transcriptions, by first running an automatic melody transcription algorithm on a (polyphonic) song to produce a series of discrete notes representing the melody, and then soliciting the crowd to correct this melody. We present a novel web-based interface that enables the crowd to correct transcriptions, report results from an experiment to understand the capabilities of non-experts to perform this challenging task, and characterize the characteristics and actions of workers and how they correlate with transcription performance. [ABSTRACT FROM AUTHOR]},
author = {Tse, Tim and Salamon, Justin and Williams, Alex and Jiang, Helga and Law, Edith},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/74546dd5e880946944f53f335eedad18482952ea.pdf:pdf},
journal = {International Society for Music Information Retrieval Conference Proceedings},
keywords = {ARRANGEMENT (Musical composition),HUMAN-machine systems,MELODY,MUSIC education,WEB-based user interfaces},
pages = {143},
title = {{Ensemble: a Hybrid Human-Machine System for Generating Melody Scores From Audio.}},
url = {http://proxy.library.nyu.edu/login?url=http://search.ebscohost.com/login.aspx?direct=true{\&}db=edb{\&}AN=123582321{\&}site=eds-live{\%}0Ahttps://steinhardt.nyu.edu/scmsAdmin/media/users/js7561/publications/tse{\_}ensemble{\_}ismir2016.pdf},
year = {2016}
}
@article{Bosch2014,
author = {Bosch, Juan J. and G{\'{o}}mez, Emilia},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/ac82b21cb020b5d82e863be1123f00b4e03b62a4.pdf:pdf},
journal = {Proceedings of the Conference on Interdisciplinary Musicology},
title = {{Melody Extraction in Symphonic Classical Music: a Comparative Study of Mutual Agreement Between Humans and Algorithms}},
url = {http://phenicx.upf.edu/system/files/publications/cim14{\_}submission{\_}114{\_}ready.pdf},
year = {2014}
}
@article{Sturm2013,
abstract = {We argue that an evaluation of system behavior at the level of the music is required to usefully address the fundamental problems of music genre recognition (MGR), and indeed other tasks of music information retrieval, such as autotagging. A recent review of works in MGR since 1995 shows that most (82 {\%}) measure the capacity of a system to recognize genre by its classification accuracy. After reviewing evaluation in MGR, we show that neither classification accuracy, nor recall and precision, nor confusion tables, necessarily reflect the capacity of a system to recognize genre in musical signals. Hence, such figures of merit cannot be used to reliably rank, promote or discount the genre recognition performance of MGR systems if genre recognition (rather than identification by irrelevant confounding factors) is the objective. This motivates the development of a richer experimental toolbox for eval- uating any system designed to intelligently extract information from music signals.},
author = {Sturm, Bob L.},
doi = {10.1007/s10844-013-0250-y},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/d3b17652e76d947f1aa583e3c99b158a792d514c.pdf:pdf},
issn = {0925-9902},
journal = {Journal of Intelligent Information Systems},
keywords = {classification,evaluation,genre,music},
number = {3},
pages = {371--406},
title = {{Classification accuracy is not enough}},
url = {https://link.springer.com/content/pdf/10.1007{\%}2Fs10844-013-0250-y.pdf},
volume = {41},
year = {2013}
}
@article{Dressler2011,
abstract = {In this paper, a new approach for pitch estimation in polyphonic musical audio is presented. The algorithm is based on the pair-wise analysis of spectral peaks. The idea of the technique lies in the identification of partials with successive (odd) harmonic numbers. Since successive partials of a harmonic sound have well defined frequency ratios, a possible fundamental can be derived from the instantaneous frequencies of the two spectral peaks. Consecutively, the identified harmonic pairs are rated according to harmonicity, timbral smoothness, the appearance of intermediate spectral peaks, and harmonic number. Finally, the resulting pitch strengths are added to a pitch spectrogram. The pitch estimation was developed for the identification of the predominant voice (e.g. melody) in polyphonic music recordings. It was evaluated as part of a melody extraction algorithm during the Music Information Retrieval Evaluation eXchange (MIREX 2006 and 2009), where the algorithm reached the best overall accuracy as well as very good performance measures.},
author = {Dressler, Karin},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/ffc05c02980f4a4ae56ebc4eb227a20ac2055e31.pdf:pdf},
journal = {42nd AES Conference},
keywords = {ch estimation by the,pair-wise evaluation},
pages = {1--10},
title = {{Pitch estimation by the pair-wise evaluation of spectral peaks}},
url = {http://www.aes.org/e-lib/browse.cfm?elib=15960},
year = {2011}
}
@article{Durrieu2011a,
abstract = {When designing an audio processing system, the target tasks often influence the choice of a data representation or transformation. Low-level time-frequency representations such as the short-time Fourier transform (STFT) are popular, because they offer a meaningful insight on sound properties for a low computational cost. Conversely, when higher level semantics, such as pitch, timbre or phoneme, are sought after, representations usually tend to enhance their discriminative characteristics, at the expense of their invertibility. They become so-called mid-level representations. In this paper, a source/filter signal model which provides a mid-level representation is proposed. This representation makes the pitch content of the signal as well as some timbre information available, hence keeping as much information from the raw data as possible. This model is successfully used within a main melody extraction system and a lead instrument/accompaniment separation system. Both frameworks obtained top results at several international evaluation campaigns.},
author = {Durrieu, Jean Louis and David, Bertrand and Richard, Gal},
doi = {10.1109/JSTSP.2011.2158801},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/820f1699024caf570047f2095009913392897519.pdf:pdf},
isbn = {1932-4553},
issn = {19324553},
journal = {IEEE Journal on Selected Topics in Signal Processing},
keywords = {Audio melody extraction,Audio signal representation,Musical audio source separation,Non-negative matrix factorization (NMF),Pitch estimation},
number = {6},
pages = {1180--1191},
title = {{A musically motivated mid-level representation for pitch estimation and musical audio source separation}},
url = {http://durrieu.ch/publis/durrieuDavidRichard{\_}musicallyMotivatedRepresentation{\_}JSTSP2011.pdf},
volume = {5},
year = {2011}
}
@article{Abeßer,
annote = {Label propagation = hled{\'{a}} nejpodobn{\v{e}}j{\v{s}}{\'{i}} spektrum v danou chv{\'{i}}li v labeled datech a pokud podobnost překro{\v{c}}{\'{i}} n{\v{e}}jakou mez, tak je olabeluje},
author = {Abe{\ss}er, Jakob and Balke, Stefan and Meinard, M and Music, Semantic and Group, Technologies and Idmt, Fraunhofer},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Abe{\ss}er et al. - Unknown - IMPROVING BASS SALIENCY ESTIMATION USING LABEL.pdf:pdf},
pages = {306--312},
title = {{Improving Bass Saliency Estimation Using Label}},
url = {http://ismir2018.ircam.fr/doc/pdfs/143{\_}Paper.pdf}
}
@article{Durrieu2010,
author = {Durrieu, Jean-louis and David, Bertrand},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Durrieu, David - 2010 - Source Filter Model for Unsupervised Main Melody.pdf:pdf},
number = {3},
pages = {1--12},
title = {{Source / Filter Model for Unsupervised Main Melody}},
url = {https://www.irit.fr/{~}Cedric.Fevotte/publications/journals/ieee{\_}asl{\_}voice{\_}extrac.pdf},
volume = {18},
year = {2010}
}
@article{Salamon2012b,
abstract = {In this paper we analyze the reliability of the evaluation of Audio Melody Extraction algorithms. We focus on the procedures and collections currently used as part of the annual Music Information Retrieval Evaluation eXchange (MIREX), which has become the de-facto benchmark for evaluating and comparing melody extraction algorithms. We study several factors: the duration of the audio clips, time offsets in the ground truth annotations, and the size and musical content of the collection. The results show that the clips currently used are too short to predict performance on full songs, highlighting the paramount need to use complete musical pieces. Concerning the ground truth, we show how a minor error, specifically a time offset between the annotation and the audio, can have a dramatic effect on the results, emphasizing the importance of establishing a common protocol for ground truth annotation and system output. We also show that results based on the small ADC04, MIREX05 and INDIAN08 collections are unreliable, while the MIREX09 collections are larger than necessary. This evidences the need for new and larger collections containing realistic music material, for reliable and meaningful evaluation of Audio Melody Extraction.},
author = {Salamon, Justin and Urbano, Juli{\'{a}}n},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/ed4e0809e19fd41ca54e8c9d1b465c0367c3857b.pdf:pdf},
isbn = {9789727521449},
journal = {Proceedings of the International Society for Music Information Retrieval Conference},
keywords = {review},
mendeley-tags = {review},
pages = {5--10},
title = {{Current Challenges in the Evaluation of Predominant Melody Extraction Algorithms}},
url = {http://mtg.upf.edu/system/files/publications/SalamonUrbanoMelodyMetaeval{\_}ISMIR12.pdf},
year = {2012}
}
@article{Goto1999,
abstract = {This paper describes a predominant-pitch estimation method that enables us to build a realtime system detecting melody and bass lines as a subsystem of our music scene description system. The purpose of this study is to build such a real-time system that is practical from the engineering viewpoint, that gives suggestions to the modeling of music understanding, and that is useful in various applications. Most previous pitch-estimation methods premised either a single-pitch sound with aperiodic noises or a few musical instruments and had great diﬃculty dealing with complex audio signals sampled from compact discs, especially discs recording jazz or popular music with drum-sounds. Our method can estimate the most predominant fundamental frequency (F0) in such signals containing sounds of various instruments because it does not rely on the F0's frequency component, which is often overlapped by other sounds' components, and instead estimates the F0 by using the Expectation-Maximization algorithm on the basis of harmonics' frequency components within an intentionally limited frequency range. It also uses a multiple-agent architecture to stably track the temporal trajectory of the F0. Experimental results show that the system is robust enough to estimate the predominant F0s of the melody and bass lines in real-world audio signals.},
author = {Goto, Masataka and Hayamizu, Satoru},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/802d415eef3ef1710ff1ddd3332339add43193fb.pdf:pdf},
journal = {IJCAI-99 Workshop on Computational Auditory Scene Analysis},
number = {August},
pages = {31--40},
title = {{A real-time music scene description system: Detecting melody and bass lines in audio signals}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.33.1085{\&}rep=rep1{\&}type=pdf},
year = {1999}
}
@article{Chan2015,
abstract = {A new algorithm is proposed for robust principal component analysis with predefined sparsity patterns. The algorithm is then applied to separate the singing voice from the instrumen-tal accompaniment using vocal activity information. To eval-uate its performance, we construct a new publicly available iKala dataset that features longer durations and higher quality than the existing MIR-1K dataset for singing voice separation. Part of it will be used in the MIREX Singing Voice Separa-tion task. Experimental results on both the MIR-1K dataset and the new iKala dataset confirmed that the more informed the algorithm is, the better the separation results are.},
author = {Chan, Tak Shing and Yeh, Tzu Chun and Fan, Zhe Cheng and Chen, Hung Wei and Su, Li and Yang, Yi Hsuan and Jang, Roger},
doi = {10.1109/ICASSP.2015.7178063},
file = {:home/jirka/07178063.pdf:pdf},
isbn = {9781467369978},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {Low-rank and sparse decomposition,informed source separation,singing voice separation},
pages = {718--722},
title = {{Vocal activity informed singing voice separation with the iKala dataset}},
volume = {2015-Augus},
year = {2015}
}
@article{Yang2017,
abstract = {Most existing neural network models for music generation use recurrent neural networks. However, the recent WaveNet model proposed by DeepMind shows that convolutional neural networks (CNNs) can also generate realistic musical waveforms in the audio domain. Following this light, we investigate using CNNs for generating melody (a series of MIDI notes) one bar after another in the symbolic domain. In addition to the generator, we use a discriminator to learn the distributions of melodies, making it a generative adversarial network (GAN). Moreover, we propose a novel conditional mechanism to exploit available prior knowledge, so that the model can generate melodies either from scratch, by following a chord sequence, or by conditioning on the melody of previous bars (e.g. a priming melody), among other possibilities. The resulting model, named MidiNet, can be expanded to generate music with multiple MIDI channels (i.e. tracks). We conduct a user study to compare the melody of eight-bar long generated by MidiNet and by Google's MelodyRNN models, each time using the same priming melody. Result shows that MidiNet performs comparably with MelodyRNN models in being realistic and pleasant to listen to, yet MidiNet's melodies are reported to be much more interesting.},
archivePrefix = {arXiv},
arxivId = {1703.10847},
author = {Yang, Li-Chia and Chou, Szu-Yu and Yang, Yi-Hsuan},
eprint = {1703.10847},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2122601e39c29b75d2f6ec057f3a72cb86feacc6.pdf:pdf},
title = {{MidiNet: A Convolutional Generative Adversarial Network for Symbolic-domain Music Generation}},
url = {http://arxiv.org/abs/1703.10847},
year = {2017}
}
@article{Goto2004,
abstract = {In this paper, we describe the concept of music scene description and address the problem of detecting melody and bass lines in real-world audio signals containing the sounds of various instruments. Most previous pitch-estimation methods have had difficulty dealing with such complex music signals because these methods were designed to deal with mixtures of only a few sounds. To enable estimation of the fundamental frequency (F0) of the melody and bass lines, we propose a predominant-F0 estimation method called PreFEst that does not rely on the unreliable fundamental component and obtains the most predominant F0 supported by harmonics within an intentionally limited frequency range. This method estimates the relative dominance of every possible F0 (represented as a probability density function of the F0) by using MAP (maximum a posteriori probability) estimation and considers the F0's temporal continuity by using a multiple-agent architecture. Experimental results with a set of ten music excerpts from compact-disc recordings showed that a real-time system implementing this method was able to detect melody and bass lines about 80{\%} of the time these existed. {\textcopyright} 2004 Elsevier B.V. All rights reserved.},
author = {Goto, Masataka},
doi = {10.1016/j.specom.2004.07.001},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/5f106136b3ce037a8963f040a4d47fa9152ccbec.pdf:pdf},
issn = {01676393},
journal = {Speech Communication},
keywords = {Computational auditory scene analysis,EM algorithm,F0 estimation,MAP estimation,Music information retrieval,Music understanding},
number = {4 SPEC. ISS.},
pages = {311--329},
title = {{A real-time music-scene-description system: Predominant-F0 estimation for detecting melody and bass lines in real-world audio signals}},
url = {http://web.cse.ohio-state.edu/{~}wang.77/teaching/cse788/papers/Goto-sc04.pdf},
volume = {43},
year = {2004}
}
@article{Tachibana2010,
abstract = {Estimation of melody line in homophonic music audio signals is a challenging subject of study. Some of the difficulties are derived from presence of accompanying components. To overcome those difficulties, we propose a method to enhance melodic components in music audio signals. The enhancement algorithm uses fluctuation and shortness of melodic components, which we call temporal-variability. We also discuss a melody tracking algorithm, which can be simple thanks to the preprocessing. In this paper, we describe the enhancement method and tracking method, and show the experimental results that supports the efficiency of our methods.},
author = {Tachibana, Hideyuki and Ono, Takuma and Ono, Nobutaka and Sagayama, Shigeki},
doi = {10.1109/ICASSP.2010.5495764},
file = {:home/jirka/Sta{\v{z}}en{\'{e}}/download.phpbibTachibana2010ICASSP03 (1).pdf:pdf},
isbn = {9781424442966},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {Harmonic-percussive sound separation,Melodic component enhancement,Melody transcription,Temporal-variability},
number = {May 2014},
pages = {425--428},
title = {{Melody line estimation in homophonic music audio signals based on temporal-variability of melodic source}},
year = {2010}
}
@article{Hawthorne2018,
abstract = {We advance the state of the art in polyphonic piano music transcription by using a deep convolutional and recurrent neural network which is trained to jointly predict onsets and frames. Our model predicts pitch onset events and then uses those predictions to condition framewise pitch predictions. During inference, we restrict the predictions from the framewise detector by not allowing a new note to start unless the onset detector also agrees that an onset for that pitch is present in the frame. We focus on improving onsets and offsets together instead of either in isolation as we believe this correlates better with human musical perception. Our approach results in over a 100{\%} relative improvement in note F1 score (with offsets) on the MAPS dataset. Furthermore, we extend the model to predict relative velocities of normalized audio which results in more natural-sounding transcriptions.},
archivePrefix = {arXiv},
arxivId = {1710.11153},
author = {Hawthorne, Curtis and Elsen, Erich and Song, Jialin and Roberts, Adam and Simon, Ian and Raffel, Colin and Engel, Jesse and Oore, Sageev and Eck, Douglas},
doi = {10.1007/s10844-013-0258-3},
eprint = {1710.11153},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/606d53680599efde9b00a14ef9d6f23fdd36903b.pdf:pdf},
issn = {09259902},
title = {{Onsets and Frames: Dual-Objective Piano Transcription}},
url = {https://arxiv.org/pdf/1710.11153.pdf http://arxiv.org/abs/1710.11153},
year = {2017}
}
@article{Stoller2018,
abstract = {Models for audio source separation usually operate on the magnitude spectrum, which ignores phase information and makes separation performance dependant on hyper-parameters for the spectral front-end. Therefore, we investigate end-to-end source separation in the time-domain, which allows modelling phase information and avoids fixed spectral transformations. Due to high sampling rates for audio, employing a long temporal input context on the sample level is difficult, but required for high quality separation results because of long-range temporal correlations. In this context, we propose the Wave-U-Net, an adaptation of the U-Net to the one-dimensional time domain, which repeatedly resamples feature maps to compute and combine features at different time scales. We introduce further architectural improvements, including an output layer that enforces source additivity, an upsampling technique and a context-aware prediction framework to reduce output artifacts. Experiments for singing voice separation indicate that our architecture yields a performance comparable to a state-of-the-art spectrogram-based U-Net architecture, given the same data. Finally, we reveal a problem with outliers in the currently used SDR evaluation metrics and suggest reporting rank-based statistics to alleviate this problem.},
archivePrefix = {arXiv},
arxivId = {1806.03185},
author = {Stoller, Daniel and Ewert, Sebastian and Dixon, Simon},
doi = {arXiv:1806.03185v1},
eprint = {1806.03185},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/cd6e8386c720d929ba46f1665617645f0291d415.pdf:pdf},
isbn = {9780874216561},
issn = {13514180},
pages = {334--340},
pmid = {15991970},
title = {{Wave-U-Net: A Multi-Scale Neural Network for End-to-End Audio Source Separation}},
url = {http://arxiv.org/abs/1806.03185},
year = {2018}
}
@article{Dressler2012,
author = {Dressler, Karin},
file = {:home/jirka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/98f72aaf8d08e2aa03ee5dc9e52228891b7763e2.pdf:pdf},
journal = {9th International Symposium on Computer Music Modeling and Retrieval (CMMR 2012)},
keywords = {auditory stream seg-,computational auditory scene analysis,melody extraction,regation},
number = {June},
pages = {19--22},
title = {{Towards Computational Auditory Scene Analysis : Melody Extraction from Polyphonic Music}},
url = {https://pdfs.semanticscholar.org/98f7/2aaf8d08e2aa03ee5dc9e52228891b7763e2.pdf},
year = {2012}
}
